{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20d0a3c",
   "metadata": {},
   "source": [
    "# SatcomLLM: Live Chat & RAG Pipeline Demo\n",
    "\n",
    "Welcome to the SatcomLLM Live Demo! This interactive notebook teaches you how to build a complete, production-ready Retrieval-Augmented Generation (RAG) system for satellite communications domain knowledge.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook demonstrates how to build a RAG system that combines semantic search with large language models to create grounded, source-attributed responses. You'll learn to store documents in a vector database, retrieve relevant context based on user queries, and generate answers using a cloud-hosted LLM.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "The demo is divided into 4 main parts:\n",
    "\n",
    "- **Setup and LLM Fundamentals**: install dependencies, configure API keys, and understand core concepts like tokenization and prompting. You'll test the connection to the RunPod vLLM endpoint to ensure everything works before building the RAG system.\n",
    "\n",
    "- **Theory around RAG**: understand the principles of Retrieval-Augmented Generation (RAG), including how retrieval complements generative models by providing up-to-date and domain-specific context.\n",
    "\n",
    "- **Embedding Vector Database Creation**: load and chunk markdown documents, generate embeddings using DeepInfra, create a Qdrant vector database collection, and populate it with embedded document chunks. This builds the knowledge base that powers semantic search.\n",
    "\n",
    "- **Retrieval and RAG Pipeline**: implement semantic search to find relevant documents, build the complete RAG pipeline that combines retrieval with generation, and test the system with real questions. You'll see how retrieved context improves answer quality and enables source attribution.\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "This system uses DeepInfra for embeddings (2560-dimensional vectors from Qwen3-4B), Qdrant Cloud for vector storage and similarity search, and RunPod vLLM for fast inference with the SatcomLLM model. Documents are processed with LangChain's header-based chunking to preserve semantic structure.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You'll need Python 3.8 or higher and API keys for DeepInfra, RunPod, and Qdrant Cloud. The complete demo takes approximately 10 minutes to run.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049b5cc",
   "metadata": {},
   "source": [
    "# PART 1: Setup & LLM Fundamentals\n",
    "\n",
    "Before building our RAG system, let's set up the environment and understand key LLM concepts.\n",
    "\n",
    "## What's in This Section?\n",
    "\n",
    "### 1. Dependency Installation\n",
    "Install all required Python packages:\n",
    "- `transformers` & `torch`: LLM libraries\n",
    "- `langchain`: RAG orchestration  \n",
    "- `qdrant-client`: Vector database\n",
    "- `requests`: API calls\n",
    "- `python-dotenv`: Environment management\n",
    "\n",
    "### 2. API Configuration\n",
    "Set up three essential services:\n",
    "- **DeepInfra**: For generating embeddings\n",
    "- **RunPod**: For LLM inference with vLLM\n",
    "- **Qdrant Cloud**: For vector storage\n",
    "\n",
    "### 3. LLM Concepts\n",
    "Learn the fundamentals:\n",
    "- **Tokenization**: Text → Numbers\n",
    "- **Prompting**: Crafting instructions\n",
    "- **Generation**: Controlling output\n",
    "\n",
    "### 4. API Testing\n",
    "Verify your RunPod vLLM connection works before building RAG.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb0f9e-1b3e-4a8c-8e66-9c70e5baf2dc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1. Setup Instructions\n",
    "\n",
    "\n",
    "First, install all required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24cd6824-878b-4c08-acfd-3f1f618c1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22281c87",
   "metadata": {},
   "source": [
    "### 1.2 Configure API Keys\n",
    "\n",
    "Copy `env.example` to `.env` and fill in your API keys.\n",
    "\n",
    "**Required API Keys:**\n",
    "\n",
    "1. **DeepInfra API Key** (for embeddings):\n",
    "   - Sign up at [deepinfra.com](https://deepinfra.com)\n",
    "   - Get your API key from the dashboard\n",
    "   - Free tier available with generous limits\n",
    "\n",
    "2. **RunPod API Key** (for LLM inference):\n",
    "   - Create account at [runpod.io](https://runpod.io)\n",
    "   - Contact [SatComLLM team](https://github.com/esa-sceva) for the endpoint URL and API key\n",
    "\n",
    "3. **Qdrant Cloud** (optional, for production):\n",
    "   - Sign up at [qdrant.tech](https://qdrant.tech)\n",
    "   - Create a cluster (free tier available)\n",
    "   - Get your cluster URL and API key\n",
    "   - *Note: The notebook will use in-memory storage if not configured*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5287d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are installed correctly\n"
     ]
    }
   ],
   "source": [
    "# Verify imports work correctly\n",
    "try:\n",
    "    import transformers\n",
    "    import langchain\n",
    "    import qdrant_client\n",
    "    import openai\n",
    "    print(\"All required packages are installed correctly\")\n",
    "except ImportError as e:\n",
    "    print(\"f Missing package: {e}\")\n",
    "    print(\"\\nPlease install dependencies:\")\n",
    "    print(\"  pip install -r requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e5984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Configuration Status:\n",
      "============================================================\n",
      "  DeepInfra (Embeddings)    Set\n",
      "  RunPod (LLM Inference)    Set\n",
      "  RunPod Endpoint URL       Set\n",
      "  Qdrant Cloud              Set\n",
      "============================================================\n",
      "All required API keys are configured!\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if required API keys are set\n",
    "print(\"Environment Configuration Status:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Required keys\n",
    "required_keys = {\n",
    "    \"DEEPINFRA_API_KEY\": \"DeepInfra (Embeddings)\",\n",
    "    \"RUNPOD_API_KEY\": \"RunPod (LLM Inference)\",\n",
    "    \"RUNPOD_API_URL\": \"RunPod Endpoint URL\"\n",
    "}\n",
    "\n",
    "all_set = True\n",
    "for key, description in required_keys.items():\n",
    "    status = \"Set\" if os.getenv(key) else \"NOT SET (REQUIRED)\"\n",
    "    print(f\"  {description:25} {status}\")\n",
    "    if not os.getenv(key):\n",
    "        all_set = False\n",
    "\n",
    "# Optional keys\n",
    "print(f\"  {'Qdrant Cloud':25} {'Set' if os.getenv('QDRANT_URL') else '○ Not set (will use in-memory)'}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_set:\n",
    "    print(\"All required API keys are configured!\")\n",
    "else:\n",
    "    print(\"\\nMissing required API keys!\")\n",
    "    print(\"   Please copy env.example to .env and add your keys.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9ed42",
   "metadata": {},
   "source": [
    "## 1.3 Tokenization: Breaking Text into Tokens\n",
    "\n",
    "Before a language model can process text, the text must be converted into a form the model can understand. This is why **tokenization** is essential: it transforms raw text into numerical units that the model can operate on. Modern LLMs do not rely on full words as atomic units; instead, they use **subword tokenization**, which divides text into smaller, reusable components called *tokens*.\n",
    "\n",
    "### Why Subword Tokenization?\n",
    "\n",
    "Subword tokenization is used because it strikes a balance between word-level and character-level representations. In particular, it offers several practical advantages:\n",
    "\n",
    "* **Robust handling of rare or unseen words**: Words that do not appear in the training vocabulary can still be represented by combining known subword units.\n",
    "* **Smaller, more efficient vocabularies**: Keeping the vocabulary compact reduces memory requirements and speeds up training and inference.\n",
    "* **Better capture of linguistic structure**: Morphological patterns (such as prefixes, suffixes, and stems) are encoded naturally through shared subword units.\n",
    "\n",
    "For example, the word *\"satellite\"* might be tokenized into `[\"sat\", \"ell\", \"ite\"]`. These components can help the model generalize to related terms such as *\"satellites\"* or even domain-specific variations like *\"satcom\"*.\n",
    "\n",
    "Let's see tokenization in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097d8bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d66d331c323495fa2f96ffda03fd673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784353baee784f868a2c7db6b9717290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47760172fae4d08801518a7719545dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7227f773ee5a4f5181fc482baba28042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Satellite communications enable global connectivity through geostationary and low Earth orbit satellites.\n",
      "\n",
      "Number of tokens: 22\n",
      "\n",
      "Tokens: ['<s>', '▁Sat', 'ellite', '▁communic', 'ations', '▁enable', '▁global', '▁connect', 'ivity', '▁through', '▁ge', 'ost', 'ation', 'ary', '▁and', '▁low', '▁Earth', '▁orbit', '▁sat', 'ell', 'ites', '.']\n",
      "\n",
      "Token IDs: [1, 12178, 20911, 7212, 800, 9025, 5534, 4511, 2068, 1549, 1737, 520, 362, 653, 322, 4482, 11563, 16980, 3290, 514, 3246, 29889]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a popular tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')\n",
    "\n",
    "# Example text about satellite communications\n",
    "text = \"Satellite communications enable global connectivity through geostationary and low Earth orbit satellites.\"\n",
    "\n",
    "# Tokenize the text\n",
    "encoded = tokenizer(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0])\n",
    "\n",
    "print(f\"Original text: {text}\\n\")\n",
    "print(f\"Number of tokens: {len(tokens)}\\n\")\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "print(f\"Token IDs: {encoded['input_ids'][0].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef89b15",
   "metadata": {},
   "source": [
    "### Understanding Token Representation\n",
    "\n",
    "Notice that:\n",
    "\n",
    "* Some words become **single tokens** (especially common words).\n",
    "* Other words are **split into multiple subword tokens** (often technical terms or rare words).\n",
    "* **Special tokens** may appear (e.g., `<s>` for start-of-sequence, `</s>` for end-of-sequence).\n",
    "* Prefixes like `Ġ` or `▁` often indicate a **leading space** before the word in certain tokenizers.\n",
    "\n",
    "Tokenization affects several aspects of LLM usage:\n",
    "\n",
    "* **Context window**: each model has a maximum number of tokens it can process (e.g., 4096, 8192…).\n",
    "* **API costs**: most LLM APIs charge based on the number of tokens processed.\n",
    "* **Inference speed**: more tokens mean slower processing and higher latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f4747",
   "metadata": {},
   "source": [
    "## 1.4 Prompting: Instructing Language Models\n",
    "\n",
    "Prompting refers to the process of designing input instructions that guide a language model to produce the desired output. The way a prompt is structured can significantly influence the quality, relevance, and accuracy of the model’s responses. In essence, effective prompting bridges the gap between human intent and model understanding.\n",
    "\n",
    "### Why Prompting Matters\n",
    "\n",
    "* **Improves response quality**: Clear and specific prompts help the model generate accurate and relevant answers.\n",
    "* **Guides model behavior**: Prompts can specify tone, style, format, or constraints for the response.\n",
    "* **Enables few-shot learning**: Including examples in the prompt allows the model to learn patterns and apply them to new queries without retraining.\n",
    "\n",
    "### Prompt Structure in Modern Chat Models\n",
    "\n",
    "Most modern chat-oriented LLMs use a structured prompt format composed of multiple message types:\n",
    "\n",
    "1. **System Message**\n",
    "   Sets the model’s role, behavior, or overall context. For example, a system message might instruct the model to act as a tutor, an assistant, or a domain expert.\n",
    "\n",
    "2. **User Message**\n",
    "   Contains the main query, instruction, or request. This is the content that the model is expected to respond to.\n",
    "\n",
    "3. **Assistant Message**\n",
    "   Represents the model’s response, or can include few-shot examples demonstrating the expected behavior. These examples help the model generalize the instruction pattern to new queries.\n",
    "\n",
    "Each message is internally encoded with special tokens that help the model distinguish between system instructions, user queries, and assistant outputs. This structure allows the model to maintain coherent multi-turn conversations and follow complex instructions more effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0115f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "<|system|>\n",
      "You are a helpful assistant specializing in satellite communications and space technology. \n",
      "Provide accurate, technical information while remaining accessible to your audience.\n",
      "<|end|>\n",
      "<|user|>\n",
      "What is the difference between GEO and LEO satellites?\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of a well-structured prompt\n",
    "prompt_template = \"\"\"<|system|>\n",
    "You are a helpful assistant specializing in satellite communications and space technology. \n",
    "Provide accurate, technical information while remaining accessible to your audience.\n",
    "<|end|>\n",
    "<|user|>\n",
    "{question}\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is the difference between GEO and LEO satellites?\"\n",
    "formatted_prompt = prompt_template.format(question=question)\n",
    "\n",
    "print(\"Formatted Prompt:\")\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34712da",
   "metadata": {},
   "source": [
    "### Prompting Best Practices\n",
    "\n",
    "1. Be Specific: Clearly state what you want\n",
    "2. Provide Context: Give relevant background information\n",
    "3. Use Examples: Few-shot prompting improves accuracy\n",
    "4. Set Constraints: Specify format, length, or style requirements\n",
    "5. System Message: Use it to set expertise domain and behavior\n",
    "\n",
    "### Key parameters for LLM text generation:\n",
    "\n",
    "  - max_tokens: Maximum number of tokens to generate (e.g., 256, 512, 1024)\n",
    "  - temperature: Controls randomness (0.0 = deterministic, 1.0 = very creative)\n",
    "  - top_p: Nucleus sampling - considers top probability mass (e.g., 0.9 = top 90%)\n",
    "  - top_k: Only consider top K most likely tokens at each step\n",
    "\n",
    "Now let's see prompting in action with a real model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e015d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokenization example:\n",
      "\n",
      "Prompt: <|system|>\n",
      "You are a helpful assistant specializing in satellite communications and space technology...\n",
      "\n",
      "Number of tokens in prompt: 80\n",
      "First 20 token IDs: [1, 529, 29989, 5205, 29989, 29958, 13, 3492, 526, 263, 8444, 20255, 4266, 5281, 297, 28421, 7212, 800, 322, 2913]\n",
      "\n",
      "We'll use RunPod vLLM for actual text generation\n"
     ]
    }
   ],
   "source": [
    "# For this notebook, we'll use RunPod vLLM for all LLM inference\n",
    "# This avoids downloading large models locally and provides production-grade performance\n",
    "\n",
    "# Let's demonstrate how the prompt would be tokenized\n",
    "print(\"Prompt tokenization example:\\n\")\n",
    "\n",
    "# Tokenize our formatted prompt from the previous cell\n",
    "prompt_tokens = tokenizer.encode(formatted_prompt)\n",
    "print(f\"Prompt: {formatted_prompt[:100]}...\")\n",
    "print(f\"\\nNumber of tokens in prompt: {len(prompt_tokens)}\")\n",
    "print(f\"First 20 token IDs: {prompt_tokens[:20]}\")\n",
    "\n",
    "# This helps us understand:\n",
    "# - How much of the model's context window the prompt uses\n",
    "# - Why API costs are often measured in tokens\n",
    "# - How to optimize prompts to fit within context limits\n",
    "\n",
    "print(\"\\nWe'll use RunPod vLLM for actual text generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7346e-4189-473c-8024-9f4084080676",
   "metadata": {},
   "source": [
    "## 1.5 Testing the RunPod vLLM API\n",
    "\n",
    "Before we proceed to RAG, it is important to ensure that our connection to the RunPod vLLM server is working correctly. This step verifies that the API endpoint is properly configured and that the SatcomLLM model hosted on the server is accessible.\n",
    "\n",
    "### RunPod vLLM Endpoint\n",
    "\n",
    "RunPod provides a scalable infrastructure for hosting large language models with low-latency inference. The vLLM API endpoint allows us to send text prompts to the hosted model and receive generated responses in real time. Testing this endpoint ensures that:\n",
    "\n",
    "* Network connectivity to the server is functional.\n",
    "* API authentication (if required) is correctly configured.\n",
    "* The hosted model is running and ready to accept requests.\n",
    "\n",
    "### Verifying the Hosted Model\n",
    "\n",
    "The model hosted on RunPod is **[LLaMA3-Satcom-8B](https://huggingface.co/esa-sceva/llama3-satcom-8b)**. By sending a test prompt to this endpoint, we can:\n",
    "\n",
    "* Confirm that the endpoint is reachable and responding correctly.\n",
    "* Ensure that the SatcomLLM model interprets queries as intended and generates coherent, domain-specific outputs.\n",
    "* Validate the setup before integrating the model into downstream workflows such as embeddings, retrieval, or RAG pipelines.\n",
    "\n",
    "Performing this test gives us confidence that the RunPod-hosted SatcomLLM is ready for production use and that our API integration is functioning properly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8fe8089-198b-4934-a655-1b4fba024e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunPod vLLM client initialized\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Get API credentials from environment\n",
    "API_URL = os.getenv(\"RUNPOD_API_URL\")\n",
    "API_KEY = os.getenv(\"RUNPOD_API_KEY\")\n",
    "\n",
    "def poll_job_status(job_id, max_attempts=30, delay=2):\n",
    "    \"\"\"Poll RunPod job status until completion.\"\"\"\n",
    "    status_url = f\"{API_URL.rsplit('/', 1)[0]}/status/{job_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        response = requests.get(status_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            status = result.get(\"status\")\n",
    "            \n",
    "            if status == \"COMPLETED\":\n",
    "                output = result.get(\"output\")\n",
    "                \n",
    "                # Parse vLLM response structure\n",
    "                if isinstance(output, list) and len(output) > 0:\n",
    "                    if \"choices\" in output[0]:\n",
    "                        choices = output[0][\"choices\"]\n",
    "                        if len(choices) > 0:\n",
    "                            # Try tokens array first (vLLM format)\n",
    "                            if \"tokens\" in choices[0]:\n",
    "                                return \"\".join(choices[0][\"tokens\"])\n",
    "                            # Try message format (OpenAI compatible)\n",
    "                            elif \"message\" in choices[0]:\n",
    "                                return choices[0][\"message\"][\"content\"]\n",
    "                \n",
    "                # OpenAI-style response\n",
    "                if isinstance(output, dict) and \"choices\" in output:\n",
    "                    return output[\"choices\"][0][\"message\"][\"content\"]\n",
    "                \n",
    "                # Direct string output\n",
    "                if isinstance(output, str):\n",
    "                    return output\n",
    "                    \n",
    "                return str(output)\n",
    "                \n",
    "            elif status == \"FAILED\":\n",
    "                return f\"Job failed: {result.get('error', 'Unknown error')}\"\n",
    "            \n",
    "            # Still running, wait and retry\n",
    "            time.sleep(delay)\n",
    "        else:\n",
    "            return f\"Status check error {response.status_code}: {response.text}\"\n",
    "    \n",
    "    return \"Timeout waiting for job completion\"\n",
    "\n",
    "def chat_with_vllm(messages, max_tokens=512, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Send chat request to RunPod vLLM server.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content'\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0 = deterministic)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"max_new_tokens\": max_tokens,  # Alternative parameter name\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        \n",
    "        # RunPod returns a job ID for async execution\n",
    "        if \"id\" in result:\n",
    "            job_id = result[\"id\"]\n",
    "            print(f\"Job submitted: {job_id}\")\n",
    "            print(\"Waiting for response...\")\n",
    "            return poll_job_status(job_id)\n",
    "        \n",
    "        # Direct response (synchronous mode)\n",
    "        if \"output\" in result:\n",
    "            return result[\"output\"]\n",
    "    \n",
    "    return f\"Error {response.status_code}: {response.text}\"\n",
    "\n",
    "print(\"RunPod vLLM client initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca6bce15-eafc-4351-b467-8f26f86d3aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RunPod vLLM API...\n",
      "Question: What is satellite communications in one sentence?\n",
      "\n",
      "Job submitted: e663c325-0223-4bde-8749-e4c4dd630e9e-e2\n",
      "Waiting for response...\n",
      "\n",
      "Response: Satellite communications is a method of exchanging information through radio signals transmitted over long distances via artificial satellites orbiting the Earth, enabling global connectivity and wireless communication between remote or hard-to-reach areas.\n",
      "\n",
      "RunPod vLLM API test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the RunPod vLLM API with a sample question\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is satellite communications in one sentence?\"}\n",
    "]\n",
    "\n",
    "print(\"Testing RunPod vLLM API...\")\n",
    "print(f\"Question: {test_messages[0]['content']}\\n\")\n",
    "\n",
    "response = chat_with_vllm(test_messages, max_tokens=256, temperature=0.7)\n",
    "\n",
    "print(f\"\\nResponse: {response}\")\n",
    "print(\"\\nRunPod vLLM API test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15590cbb-fc6b-422d-8176-a0435ae96b27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: THEORY AROUND RAG SYSTEM\n",
    "\n",
    "Now that we know that the model is working on the RunPod endpoint and can answer questions, let's understand how to implement RAG, starting from **custom documents**\n",
    "But before we build, let's understand why and how use RAG.\n",
    "\n",
    "## 2.1. The Problem with Standard LLMs\n",
    "\n",
    "Large Language Models have limitations:\n",
    "\n",
    "| Issue | Description | Impact |\n",
    "|-------|-------------|--------|\n",
    "| **Hallucinations** | Generate plausible but false info | Unreliable answers |\n",
    "| **Knowledge Cutoff** | Training data has a date limit | Outdated information |\n",
    "| **No Source** | Can't cite where info came from | Unverifiable |\n",
    "| **Generic** | Lack domain-specific expertise | Poor specialized answers |\n",
    "| **Static** | Can't update without retraining | Expensive to maintain |\n",
    "\n",
    "## How RAG Solves These Problems\n",
    "\n",
    "**Retrieval-Augmented Generation** adds a knowledge retrieval step:\n",
    "\n",
    "```\n",
    "Standard LLM:\n",
    "Question → LLM → Answer (may hallucinate)\n",
    "\n",
    "RAG System:\n",
    "Question → Find Relevant Docs → LLM + Context → Grounded Answer\n",
    "```\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- Grounded: Answers based on actual documents  \n",
    "- Verifiable: Shows sources used  \n",
    "- Up-to-date: Update docs without retraining model  \n",
    "- Domain-specific: Add specialized knowledge  \n",
    "- Cost-effective: Cheaper than fine-tuning  \n",
    "\n",
    "## 2.2 RAG Architecture\n",
    "\n",
    "### Two Main Phases:\n",
    "\n",
    "#### Phase 1: Indexing (One-Time Setup)\n",
    "```\n",
    "Documents → Clean → Chunk → Embed → Store in Vector DB\n",
    "```\n",
    "This is what we'll do in this part!\n",
    "\n",
    "#### Phase 2: Query (Every Question)\n",
    "```\n",
    "Question → Embed → Search Vector DB → Retrieve Docs → \n",
    "    Augment Prompt → LLM → Answer\n",
    "```\n",
    "This is what we will implement in part 3!\n",
    "\n",
    "## 2.3 When to Use RAG\n",
    "\n",
    "| Use Case | RAG? | Why |\n",
    "|----------|------|-----|\n",
    "| Documentation Q&A | Yes | Need exact info from docs |\n",
    "| Technical support | Yes | Must cite solutions |\n",
    "| General chat | No | Model knowledge sufficient |\n",
    "| Research queries | Yes | Need to reference papers |\n",
    "| Identity info | No | Model can hallucinate |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e45f75",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4 What We'll Build\n",
    "\n",
    "In the following cells, we will:\n",
    "\n",
    "1. Load and Chunk Documents - Process satcom docs into semantic chunks\n",
    "2. Set Up Embeddings - Embed the chunks for retrieving tasks, before populating the vector database.  \n",
    "3. Create Vector Database on Qdrant\n",
    "4. Upload Knowledge Base - Store all document chunks in the vector DB\n",
    "5. Implement Search - Semantic retrieval from the vector database\n",
    "6. Build RAG Pipeline - Combine retrieval with vLLM generation\n",
    "7. Test & Interact - Ask questions and get grounded answers\n",
    "\n",
    "## 2.5 Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────┐\n",
    "│   User      │ Asks a question\n",
    "│  Question   │\n",
    "└──────┬──────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  1. EMBED QUERY (DeepInfra)              │\n",
    "│     Convert question → 2560-dim vector   │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  2. SEMANTIC SEARCH (Qdrant Cloud)       │\n",
    "│     Find top-k similar document chunks   │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  3. RETRIEVE CONTEXT                     │\n",
    "│     Get text + metadata from matches     │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  4. ENRICH PROMPT                        │\n",
    "│     Question + Retrieved Context         │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  5. GENERATE ANSWER (RunPod vLLM)        │\n",
    "│     SatcomLLM produces grounded response │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────┐\n",
    "│   Answer     │ With source attribution\n",
    "│ + Sources    │\n",
    "└──────────────┘\n",
    "```\n",
    "\n",
    "\n",
    "Let's start building!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bba21",
   "metadata": {},
   "source": [
    "# Part 3: Embedding & Vector Database\n",
    "\n",
    "In a Retrieval-Augmented Generation (RAG) system, as we have seen before, the knowledge base is represented in a way that allows fast and semantically meaningful search. This is achieved by **embedding** text into high-dimensional vectors and storing them in a **vector database**.\n",
    "\n",
    "In this part of the pipeline, we will:\n",
    "\n",
    "1. **Load documents** from local files (Markdown or converted PDFs).\n",
    "2. **Chunk the documents** intelligently to preserve semantic and hierarchical structure.\n",
    "3. **Generate embeddings** for each chunk using a specialized embedding model.\n",
    "4. **Upload the embeddings to a vector database**, making them ready for retrieval during RAG.\n",
    "\n",
    "\n",
    "\n",
    "## 3.1. Document Loading and Chunking\n",
    "\n",
    "Before we can build our RAG system, we need to prepare our knowledge base. This involves:\n",
    "\n",
    "### Document Loading\n",
    "We'll use a couple of Markdown files in the `data` folder as our example of knowledge source, any other kind of `.md` file is fine. We will also explore how to easily convert `pdfs` into `.md` using a simple python library if needed.\n",
    "\n",
    "### Intelligent Chunking\n",
    "Then we use header-based markdown splitting (hierarchical chunking) to preserve semantic structure:\n",
    "- Split on markdown headers (#, ##, ###, ####)\n",
    "- Maintain parent header context in metadata\n",
    "- Respect maximum chunk size (1000 characters)\n",
    "- Add overlap between chunks for continuity\n",
    "\n",
    "This approach is inspired by the SatcomLLM pipeline's own chunking strategies and ensures:\n",
    "- Semantic coherence - Chunks respect document structure\n",
    "- Context preservation - Headers provide hierarchical context  \n",
    "- Optimal retrieval - Chunks are sized for effective embedding\n",
    "\n",
    "Run the next cells to load and chunk documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a2d098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 markdown file(s) in 'data':\n",
      "  - 01382b63-a19b-46af-b9ab-21e399f22d09.md\n",
      "  - 0100a359-2176-4af9-8fc8-8b8a16988c33.md\n",
      "  - 00ee777f-df00-4e9e-8493-d0356ffafef9.md\n",
      "  - 00b5dfe7-904b-493b-bac8-b7ed98a65338.md\n",
      "\n",
      "01382b63-a19b-46af-b9ab-21e399f22d09.md: 69292 characters\n",
      "\n",
      "0100a359-2176-4af9-8fc8-8b8a16988c33.md: 25219 characters\n",
      "\n",
      "00ee777f-df00-4e9e-8493-d0356ffafef9.md: 53416 characters\n",
      "\n",
      "00b5dfe7-904b-493b-bac8-b7ed98a65338.md: 47048 characters\n",
      "\n",
      "Total characters across all documents: 194975\n",
      "\n",
      "Preview of first document:\n",
      "Deep Learning-based Joint Channel Prediction and Multibeam Precoding for LEO Satellite Internet of Things\n",
      "\n",
      "[PERSON], [PERSON], [PERSON], and [PERSON]\n",
      "\n",
      "Part of this paper has been presented at the IEEE ICCC 2023 [1].[PERSON] and [PERSON] are with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China (e-mail:{ming_ying_, [EMAIL_ADDRESS]). [PERSON] is with the School of Information Science and Technology, Hangzhou Normal University, Hangzhou, 31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all markdown documents from the data folder\n",
    "data_folder = Path('data')\n",
    "\n",
    "# Check if data folder exists\n",
    "if not data_folder.exists():\n",
    "    raise FileNotFoundError(f\"Data folder not found: {data_folder}\")\n",
    "\n",
    "# Find all markdown files\n",
    "markdown_files = list(data_folder.glob('*.md')) + list(data_folder.glob('*.markdown'))\n",
    "\n",
    "if not markdown_files:\n",
    "    raise FileNotFoundError(f\"No markdown files found in {data_folder}\")\n",
    "\n",
    "print(f\"Found {len(markdown_files)} markdown file(s) in '{data_folder}':\")\n",
    "for f in markdown_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Load all documents\n",
    "documents = {}\n",
    "total_chars = 0\n",
    "\n",
    "for file_path in markdown_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        documents[file_path.name] = content\n",
    "        total_chars += len(content)\n",
    "        print(f\"\\n{file_path.name}: {len(content)} characters\")\n",
    "\n",
    "print(f\"\\nTotal characters across all documents: {total_chars}\")\n",
    "print(f\"\\nPreview of first document:\")\n",
    "first_doc = list(documents.values())[0]\n",
    "print(first_doc[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f8bab",
   "metadata": {},
   "source": [
    "We'll use LangChain's `MarkdownHeaderTextSplitter` which aligns with the SatcomLLM chunking strategy from `synthetic_gen/chunker/`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86bba3bf-895c-4ab5-9a85-fad0a2cf0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def chunk_markdown_document(markdown_text, max_chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Chunk markdown document using header-based splitting.\n",
    "    This approach is inspired by the SatcomLLM synthetic data generation pipeline.\n",
    "    \n",
    "    Args:\n",
    "        markdown_text: Markdown formatted text\n",
    "        max_chunk_size: Maximum chunk size in characters\n",
    "        chunk_overlap: Overlap between chunks for context preservation\n",
    "    \n",
    "    Returns:\n",
    "        List of Document objects with content and metadata\n",
    "    \"\"\"\n",
    "    # Define headers to split on (matching SatcomLLM approach)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    # Initialize markdown splitter\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "    \n",
    "    # Split by headers\n",
    "    md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "    \n",
    "    # Further split large chunks if needed\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=max_chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Process each header-based chunk\n",
    "    all_chunks = []\n",
    "    for idx, doc in enumerate(md_header_splits):\n",
    "        # If chunk is too large, split further\n",
    "        if len(doc.page_content) > max_chunk_size:\n",
    "            sub_chunks = text_splitter.split_text(doc.page_content)\n",
    "            for sub_idx, sub_chunk in enumerate(sub_chunks):\n",
    "                chunk_doc = Document(\n",
    "                    page_content=sub_chunk,\n",
    "                    metadata={\n",
    "                        **doc.metadata,\n",
    "                        'chunk_id': f\"{idx}_{sub_idx}\",\n",
    "                        'chunk_size': len(sub_chunk)\n",
    "                    }\n",
    "                )\n",
    "                all_chunks.append(chunk_doc)\n",
    "        else:\n",
    "            doc.metadata['chunk_id'] = str(idx)\n",
    "            doc.metadata['chunk_size'] = len(doc.page_content)\n",
    "            all_chunks.append(doc)\n",
    "    \n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "896bdcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 01382b63-a19b-46af-b9ab-21e399f22d09.md...\n",
      "  Generated 103 chunks\n",
      "\n",
      "Processing 0100a359-2176-4af9-8fc8-8b8a16988c33.md...\n",
      "  Generated 31 chunks\n",
      "\n",
      "Processing 00ee777f-df00-4e9e-8493-d0356ffafef9.md...\n",
      "  Generated 73 chunks\n",
      "\n",
      "Processing 00b5dfe7-904b-493b-bac8-b7ed98a65338.md...\n",
      "  Generated 69 chunks\n",
      "\n",
      "============================================================\n",
      "Total chunks across all documents: 276\n",
      "============================================================\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Source: 01382b63-a19b-46af-b9ab-21e399f22d09.md\n",
      "Headers:  > \n",
      "Content preview: Deep Learning-based Joint Channel Prediction and Multibeam Precoding for LEO Satellite Internet of Things  \n",
      "[PERSON], [PERSON], [PERSON], and [PERSON]...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Source: 01382b63-a19b-46af-b9ab-21e399f22d09.md\n",
      "Headers:  > \n",
      "Content preview: Low earth orbit (LEO) satellite internet of things (IoT) is a promising way achieving global Internet of Everything, and thus has been widely recogniz...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Source: 01382b63-a19b-46af-b9ab-21e399f22d09.md\n",
      "Headers:  > \n",
      "Content preview: predicts the CSI of current time slot according to that of previous time slots. With the predicted CSI, this paper designs a DL-based robust multibeam...\n"
     ]
    }
   ],
   "source": [
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "\n",
    "for filename, markdown_text in documents.items():\n",
    "    print(f\"\\nProcessing {filename}...\")\n",
    "    doc_chunks = chunk_markdown_document(markdown_text)\n",
    "    \n",
    "    # Add source filename to metadata\n",
    "    for chunk in doc_chunks:\n",
    "        chunk.metadata['source_file'] = filename\n",
    "    \n",
    "    all_chunks.extend(doc_chunks)\n",
    "    print(f\"  Generated {len(doc_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total chunks across all documents: {len(all_chunks)}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show sample chunks from different documents\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Source: {chunk.metadata.get('source_file', 'unknown')}\")\n",
    "    print(f\"Headers: {chunk.metadata.get('Header 1', '')} > {chunk.metadata.get('Header 2', '')}\")\n",
    "    print(f\"Content preview: {chunk.page_content[:150]}...\")\n",
    "\n",
    "# Rename for consistency with rest of notebook\n",
    "chunks = all_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04ffc7",
   "metadata": {},
   "source": [
    "## 3.2 Initialize DeepInfra Embeddings\n",
    "\n",
    "Now we'll set up our embedding model using **DeepInfra's API**.\n",
    "\n",
    "### What are Embeddings?\n",
    "Embeddings convert text into dense numerical vectors that capture semantic meaning:\n",
    "- Similar texts → Similar vectors\n",
    "- Enable semantic search (not just keyword matching)\n",
    "- Foundation for RAG retrieval\n",
    "\n",
    "### Model Specifications\n",
    "- **Model**: `Qwen/Qwen3-Embedding-4B` \n",
    "- **Dimensions**: 2560\n",
    "- **Use Case**: High-quality semantic search and retrieval\n",
    "- **Normalization**: Vectors are normalized for cosine similarity\n",
    "\n",
    "Run the next cell to initialize the embedding model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9ce864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DeepInfra embeddings with corrected settings...\n",
      "✓ DeepInfra embeddings initialized successfully!\n",
      "✓ Model: Qwen/Qwen3-Embedding-4B\n",
      "✓ Embedding dimension: 2560\n",
      "✓ Sample embedding (first 10 values): [-0.0002994537353515625, 0.0308837890625, -0.0194091796875, 0.016357421875, -0.00115966796875, 0.058349609375, 0.060302734375, 0.000591278076171875, 0.023193359375, -0.006500244140625]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Fixed DeepInfra Embeddings API wrapper\n",
    "class DeepInfraEmbeddings:\n",
    "    \"\"\"Embeddings using DeepInfra API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, model=\"Qwen/Qwen3-Embedding-4B\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.api_url = \"https://api.deepinfra.com/v1/openai/embeddings\"  # Fixed URL\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embed a list of documents\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"input\": texts\n",
    "        }\n",
    "        response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return [item[\"embedding\"] for item in result[\"data\"]]\n",
    "        else:\n",
    "            raise Exception(f\"DeepInfra API error: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embed a single query\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# Initialize DeepInfra embedding model with FIXED settings\n",
    "print(\"Initializing DeepInfra embeddings with corrected settings...\")\n",
    "embedding_model = DeepInfraEmbeddings(\n",
    "    api_key=os.getenv(\"DEEPINFRA_API_KEY\"),\n",
    "    model=\"Qwen/Qwen3-Embedding-4B\"  # Known working model, 1024 dimensions\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "sample_text = \"The SatcomLLM pipeline generates synthetic QA pairs from documents\"\n",
    "sample_embedding = embedding_model.embed_query(sample_text)\n",
    "\n",
    "print(f\"✓ DeepInfra embeddings initialized successfully!\")\n",
    "print(f\"✓ Model: {embedding_model.model}\")\n",
    "print(f\"✓ Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"✓ Sample embedding (first 10 values): {sample_embedding[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330a2fc",
   "metadata": {},
   "source": [
    "## 3.3 Create Qdrant Vector Database Collection\n",
    "\n",
    "Time to set up our **vector database** where we'll store all document embeddings for fast retrieval.\n",
    "\n",
    "### What is Qdrant?\n",
    "**Qdrant** is a high-performance vector database designed for:\n",
    "- **Similarity Search**: Find semantically similar documents in milliseconds\n",
    "- **Scalability**: Handle millions of vectors efficiently\n",
    "- **Filtering**: Combine vector search with metadata filters\n",
    "- **Production-Ready**: Built for real-world applications\n",
    "\n",
    "### Collection Configuration\n",
    "\n",
    "We're creating a collection with these specifications:\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|-----------|-------|-------------|\n",
    "| **Name** | `satcom_rag_demo` | Identifier for our knowledge base |\n",
    "| **Vector Size** | 2560 | Matches Qwen3-Embedding-4B output |\n",
    "| **Distance Metric** | Cosine | Best for normalized embeddings |\n",
    "| **Storage** | Qdrant Cloud | Persistent, managed storage |\n",
    "\n",
    "\n",
    "Run the next cell to create the collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4b8f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Qdrant Cloud...\n",
      "Creating collection: satcom_rag_demo\n",
      "✓ Collection 'satcom_rag_demo' created successfully!\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "\n",
    "# Initialize Qdrant Cloud client\n",
    "print(\"Connecting to Qdrant Cloud...\")\n",
    "qdrant_client = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_URL\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "# Collection name\n",
    "collection_name = \"satcom_rag_demo\"\n",
    "\n",
    "# Create new collection\n",
    "print(f\"Creating collection: {collection_name}\")\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=2560, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "print(f\"✓ Collection '{collection_name}' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d24f04",
   "metadata": {},
   "source": [
    "And upload chunks created to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073883f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and uploading 276 chunks to Qdrant...\n",
      "This may take a moment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 28/28 [00:42<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading 276 points to Qdrant...\n",
      "✓ Successfully uploaded 276 chunks to Qdrant!\n",
      "\n",
      "Collection info:\n",
      "  - Points count: 276\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Upload chunks to Qdrant with embeddings\n",
    "print(f\"Embedding and uploading {len(chunks)} chunks to Qdrant...\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "# Prepare points for batch upload\n",
    "points = []\n",
    "\n",
    "# Process chunks in batches to avoid API rate limits\n",
    "batch_size = 10\n",
    "for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing chunks\"):\n",
    "    batch_chunks = chunks[i:i+batch_size]\n",
    "    \n",
    "    # Get texts from batch\n",
    "    texts = [chunk.page_content for chunk in batch_chunks]\n",
    "    \n",
    "    # Generate embeddings for batch\n",
    "    embeddings = embedding_model.embed_documents(texts)\n",
    "    \n",
    "    # Create points\n",
    "    for j, (chunk, embedding) in enumerate(zip(batch_chunks, embeddings)):\n",
    "        point_id = str(uuid.uuid4())\n",
    "        points.append(\n",
    "            PointStruct(\n",
    "                id=point_id,\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"text\": chunk.page_content,\n",
    "                    \"metadata\": chunk.metadata,\n",
    "                    \"chunk_id\": chunk.metadata.get(\"chunk_id\", f\"{i+j}\"),\n",
    "                    \"source\": chunk.metadata.get(\"source_file\")\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Upload all points to Qdrant\n",
    "print(f\"\\nUploading {len(points)} points to Qdrant...\")\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(f\"✓ Successfully uploaded {len(points)} chunks to Qdrant!\")\n",
    "\n",
    "# Verify collection info\n",
    "collection_info = qdrant_client.get_collection(collection_name=collection_name)\n",
    "print(f\"\\nCollection info:\")\n",
    "print(f\"  - Points count: {collection_info.points_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806cd55",
   "metadata": {},
   "source": [
    "# Part 4: Retrieval, Prompt Enrichment, and Generation\n",
    "\n",
    "In this part, we move from preparing the knowledge base to actively using it in a RAG workflow. The goal is to answer user queries by:\n",
    "\n",
    "1. Retrieving relevant document chunks from the vector database using semantic similarity search.\n",
    "2. Enriching the prompt by combining the retrieved context with the user query, ensuring the model has all the necessary information.\n",
    "3. Generating responses with the RunPod vLLM-hosted SatcomLLM model, producing accurate and context-aware answers.\n",
    "\n",
    "\n",
    "This step effectively demonstrates the full RAG loop: from query to retrieval, prompt construction, and finally, high-quality answer generation using a domain-specific LLM. By combining semantic search with prompt engineering, we ensure that even complex or technical queries are answered accurately and coherently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfcf4c",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1: Implement Semantic Search\n",
    "\n",
    "With our knowledge base populated, first let's build the **search function** that retrieves the most relevant document chunks for any user query.\n",
    "\n",
    "#### How Semantic Search Works\n",
    "\n",
    "```\n",
    "User Question\n",
    "    ↓\n",
    "1. Embed question using DeepInfra\n",
    "    ↓\n",
    "2. Query Qdrant with question vector\n",
    "    ↓\n",
    "3. Qdrant finds similar document vectors\n",
    "    ↓\n",
    "4. Return top-k most similar chunks\n",
    "    ↓\n",
    "Retrieved Documents + Similarity Scores\n",
    "```\n",
    "\n",
    "#### Understanding Similarity Scores\n",
    "\n",
    "The **cosine similarity score** ranges from 0 to 1:\n",
    "- **0.9 - 1.0**: Extremely similar (near-duplicate)\n",
    "- **0.7 - 0.9**: Highly relevant (strong match)\n",
    "- **0.5 - 0.7**: Moderately relevant (partial match)\n",
    "- **< 0.5**: Weakly relevant (consider filtering out)\n",
    "\n",
    "#### Function Features\n",
    "- **Top-K Retrieval**: Get the k most similar documents\n",
    "- **Score Transparency**: See exactly how relevant each result is\n",
    "- **Metadata Preservation**: Returns headers and structure info\n",
    "- **Fast**: Typically < 100ms for searches\n",
    "\n",
    "Run the next cell to implement and test the search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4828427a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: What is a LEO Satellite?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Result 1 (Score: 0.7069) ---\n",
      "Section:  > I Introduction\n",
      "Text preview: Recently, low-earth-orbit (LEO) satellite with low transmission latency and small propagation loss has arisen public interest by its potential as an expansion of traditional terrestrial networks to address the above issues. It can provide continuous and sufficient connectivity for districts without ...\n",
      "\n",
      "--- Result 2 (Score: 0.6497) ---\n",
      "Section:  > \n",
      "Text preview: Deep learning, multibeam precoding, channel prediction, LEO satellite Internet of Things....\n",
      "\n",
      "--- Result 3 (Score: 0.6066) ---\n",
      "Section:  > I Introduction\n",
      "Text preview: The remainder of this article is organized as follows. In Section II, we introduce the channel model and channel estimation method for LEO satellite IoT. Based on the system model, we propose a supervised channel prediction scheme to predict the current CSI using the CSI of previous time slots in Se...\n",
      "\n",
      "================================================================================\n",
      "✓ Search function working correctly!\n"
     ]
    }
   ],
   "source": [
    "def search_knowledge_base(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search the knowledge base for relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with scores\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    \n",
    "    # Search in Qdrant using the correct API method\n",
    "    search_results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for result in search_results.points:\n",
    "        results.append({\n",
    "            \"text\": result.payload[\"text\"],\n",
    "            \"metadata\": result.payload[\"metadata\"],\n",
    "            \"score\": result.score\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the search function\n",
    "test_query = \"What is a LEO Satellite?\"\n",
    "print(f\"Test Query: {test_query}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = search_knowledge_base(test_query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} (Score: {result['score']:.4f}) ---\")\n",
    "    print(f\"Section: {result['metadata'].get('Header 1', '')} > {result['metadata'].get('Header 2', '')}\")\n",
    "    print(f\"Text preview: {result['text'][:300]}...\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Search function working correctly!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5bceb",
   "metadata": {},
   "source": [
    "## 4.2 Complete RAG Pipeline\n",
    "\n",
    "Now we bring everything together! This is the **main RAG function** that combines retrieval with generation.\n",
    "\n",
    "#### The Complete RAG Workflow\n",
    "\n",
    "#### Phase 1: Retrieval\n",
    "```\n",
    "Question → Embed → Search Qdrant → Top-K Documents\n",
    "```\n",
    "- Convert user question to vector\n",
    "- Find most similar chunks in vector database  \n",
    "- Retrieve actual text content and metadata\n",
    "\n",
    "#### Phase 2: Context Building\n",
    "```\n",
    "Retrieved Docs → Format with Headers → Enriched Context\n",
    "```\n",
    "- Format each document with its section headers\n",
    "- Include document numbers for reference\n",
    "- Combine into structured context string\n",
    "\n",
    "#### Phase 3: Prompt Engineering\n",
    "```\n",
    "System Prompt + Context + Question → Enriched Prompt\n",
    "```\n",
    "- Instruct the model to use provided context\n",
    "- Include all retrieved documents as context\n",
    "- Add the user's original question\n",
    "- Set clear expectations for the response\n",
    "\n",
    "#### Phase 4: Generation\n",
    "```\n",
    "Enriched Prompt → RunPod vLLM → Grounded Answer\n",
    "```\n",
    "- Send to SatcomLLM model on RunPod\n",
    "- vLLM provides fast, optimized inference\n",
    "- Model generates answer based on context\n",
    "- Returns answer with source attribution\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|---------|\n",
    "| **Grounded** | Answers based on actual documents, not hallucinations |\n",
    "| **Traceable** | Shows which documents were used |\n",
    "| **Transparent** | Displays similarity scores for trust |\n",
    "| **Configurable** | Adjust top_k, temperature, max_tokens |\n",
    "| **Fast** | vLLM optimization for quick responses |\n",
    "\n",
    "### Function Parameters\n",
    "\n",
    "- `question`: Your query (string)\n",
    "- `top_k`: Number of documents to retrieve (default: 3)\n",
    "- `max_tokens`: Maximum answer length (default: 512)\n",
    "- `temperature`: Creativity (0.0 = factual, 1.0 = creative)\n",
    "- `show_sources`: Display retrieved documents (default: True)\n",
    "\n",
    "Run the next cell to implement the complete RAG pipeline and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e59b28e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Complete RAG Pipeline\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question: Tell me about Deep Learning-based Joint Channel Prediction and Multibeam Precoding for LEO Satellite Internet of Things\n",
      "================================================================================\n",
      "\n",
      "Searching knowledge base...\n",
      "\n",
      "Retrieved 3 relevant documents:\n",
      "  [1]  (score: 0.936)\n",
      "  [2]  (score: 0.901)\n",
      "  [3]  (score: 0.852)\n",
      "\n",
      "Generating answer with SatcomLLM...\n",
      "Job submitted: ba98ab9e-425f-46ac-bbdc-47a13ee3c57f-e2\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Based on the provided context, here's a summary of Deep Learning-based Joint Channel Prediction and Multibeam Precoding for LEO Satellite Internet of Things:\n",
      "\n",
      "- **Purpose**: Develop a deep learning (DL) based joint channel prediction and multibeam precoding scheme to address the challenges of high-speed movement of Low Earth Orbit (LEO) satellites, such as acquiring timely channel state information (CSI) and designing effective multibeam precoding for various IoT applications.\n",
      "\n",
      "- **Methods**:\n",
      " \n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, here's a summary of Deep Learning-based Joint Channel Prediction and Multibeam Precoding for LEO Satellite Internet of Things:\\n\\n- **Purpose**: Develop a deep learning (DL) based joint channel prediction and multibeam precoding scheme to address the challenges of high-speed movement of Low Earth Orbit (LEO) satellites, such as acquiring timely channel state information (CSI) and designing effective multibeam precoding for various IoT applications.\\n\\n- **Methods**:\\n \""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_rag_question(question, top_k=3, max_tokens=512, temperature=0.1, show_sources=True):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieve relevant documents and generate answer using vLLM.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        top_k: Number of documents to retrieve\n",
    "        max_tokens: Maximum tokens for generation\n",
    "        temperature: Sampling temperature\n",
    "        show_sources: Whether to display retrieved sources\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    print(\"Searching knowledge base...\")\n",
    "    retrieved_docs = search_knowledge_base(question, top_k=top_k)\n",
    "    \n",
    "    if show_sources:\n",
    "        print(f\"\\nRetrieved {len(retrieved_docs)} relevant documents:\")\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            headers = doc['metadata'].get('Header 1', '')\n",
    "            if doc['metadata'].get('Header 2'):\n",
    "                headers += f\" > {doc['metadata'].get('Header 2')}\"\n",
    "            print(f\"  [{i}] {headers} (score: {doc['score']:.3f})\")\n",
    "    \n",
    "    # Step 2: Format context from retrieved documents\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        headers = []\n",
    "        if doc['metadata'].get('Header 1'):\n",
    "            headers.append(doc['metadata']['Header 1'])\n",
    "        if doc['metadata'].get('Header 2'):\n",
    "            headers.append(doc['metadata']['Header 2'])\n",
    "        \n",
    "        section_info = \" > \".join(headers) if headers else \"General\"\n",
    "        context_parts.append(f\"[Document {i}] {section_info}\\n{doc['text']}\\n\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Step 3: Build enriched prompt\n",
    "    enriched_prompt = f\"\"\"You are a helpful assistant specializing in satellite communications.\n",
    "\n",
    "Use the following context from the documentation to answer the question accurately and concisely.\n",
    "If the answer cannot be found in the context, say so clearly.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer using vLLM\n",
    "    print(\"\\nGenerating answer with SatcomLLM...\")\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": enriched_prompt}\n",
    "    ]\n",
    "    \n",
    "    answer = chat_with_vllm(messages, max_tokens=max_tokens, temperature=temperature)\n",
    "    \n",
    "    # Display answer\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"ANSWER:\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(answer)\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the complete RAG pipeline\n",
    "print(\"Testing Complete RAG Pipeline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test question 1\n",
    "ask_rag_question(\n",
    "    \"Tell me about Deep Learning-based Joint Channel Prediction and Multibeam Precoding for LEO Satellite Internet of Things\",\n",
    "    top_k=3,\n",
    "    temperature=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734187a9",
   "metadata": {},
   "source": [
    "## 4.3 Testing with custom questions\n",
    "\n",
    "Let's test our RAG system with several diverse questions about the SatcomLLM pipeline to demonstrate its capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03a6b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Question: What is a LEO satellite?\n",
      "================================================================================\n",
      "\n",
      "Searching knowledge base...\n",
      "\n",
      "Retrieved 3 relevant documents:\n",
      "  [1]  > I Introduction (score: 0.710)\n",
      "  [2]  (score: 0.634)\n",
      "  [3]  > I Introduction (score: 0.607)\n",
      "\n",
      "Generating answer with SatcomLLM...\n",
      "Job submitted: cc72930e-7c1a-4856-94da-42c95e7dfc6f-e1\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "According to Document 1, a Low-Earth-Orbit (LEO) satellite has the following characteristics: \n",
      "\n",
      "It has low transmission latency and small propagation loss. It is used for continuous and sufficient connectivity for areas without adequate network coverage.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question: What is deep learning?\n",
      "================================================================================\n",
      "\n",
      "Searching knowledge base...\n",
      "\n",
      "Retrieved 3 relevant documents:\n",
      "  [1]  (score: 0.579)\n",
      "  [2]  > III DL-Based Channel Prediction (score: 0.512)\n",
      "  [3]  > III DL-Based Channel Prediction (score: 0.481)\n",
      "\n",
      "Generating answer with SatcomLLM...\n",
      "Job submitted: 49a8a13d-521e-46a1-a0da-a7596cbaf33c-e1\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Deep learning is not explicitly explained in the provided context, but based on general satellite communication knowledge, deep learning can be defined as a subfield of machine learning that uses neural networks with multiple hidden layers to learn complex patterns in data. This is a broad definition, as deep learning is not specifically discussed in the provided documents.\n",
      "\n",
      "However, since you provided 'Deep learning, multibeam precoding, channel prediction, LEO satellite Internet of Things' as an additional context, I can infer that in this\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question: What is Internet of Things?\n",
      "================================================================================\n",
      "\n",
      "Searching knowledge base...\n",
      "\n",
      "Retrieved 3 relevant documents:\n",
      "  [1]  > I Introduction (score: 0.557)\n",
      "  [2]  (score: 0.513)\n",
      "  [3]  > II System Model (score: 0.508)\n",
      "\n",
      "Generating answer with SatcomLLM...\n",
      "Job submitted: 1f4b1bfd-b91f-4eeb-a84e-8911c0b53488-e2\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "The Internet of Things (IoT) is a network of billions of connected devices that collect and exchange data in various applications such as smart cities, intelligent transportation, and automatic environmental monitoring.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with more questions\n",
    "test_questions = [\n",
    "    \"What is a LEO satellite?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"What is Internet of Things?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    ask_rag_question(question, top_k=3, temperature=0.1)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec5cc2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Question: What is SatCom?\n",
      "================================================================================\n",
      "\n",
      "Searching knowledge base...\n",
      "\n",
      "Retrieved 3 relevant documents:\n",
      "  [1]  (score: 0.461)\n",
      "  [2]  > I Introduction (score: 0.457)\n",
      "  [3]  > I Introduction (score: 0.455)\n",
      "\n",
      "Generating answer with SatcomLLM...\n",
      "Job submitted: 5dba8737-2940-4554-a04e-3019d38116e7-e1\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Based on your provided context, specifically from [Document 1] General and [Document 3] I Introduction, I cannot find the explicit definition of \"SatCom\" in the given information. However, it is probable that \"SatCom\" refers to Satellite Communications.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on your provided context, specifically from [Document 1] General and [Document 3] I Introduction, I cannot find the explicit definition of \"SatCom\" in the given information. However, it is probable that \"SatCom\" refers to Satellite Communications.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask your own question!\n",
    "my_question = \"What is SatCom?\"\n",
    "\n",
    "# Adjust parameters as needed:\n",
    "# - top_k: Number of documents to retrieve (3-5 recommended)\n",
    "# - temperature: 0.0 = deterministic, 1.0 = creative\n",
    "# - max_tokens: Maximum length of answer\n",
    "ask_rag_question(\n",
    "    question=my_question,\n",
    "    top_k=3,\n",
    "    temperature=0.1,\n",
    "    max_tokens=512,\n",
    "    show_sources=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021a56",
   "metadata": {},
   "source": [
    "## Summary: Complete RAG Pipeline\n",
    "\n",
    "Congratulations! You've built a complete RAG (Retrieval-Augmented Generation) system with:\n",
    "\n",
    "### Components:\n",
    "1. **Document Processing**: Markdown chunking with header-based splitting\n",
    "2. **Embeddings**: DeepInfra API with BAAI/bge-large-en-v1.5 (1024 dimensions)\n",
    "3. **Vector Database**: Qdrant Cloud for scalable similarity search\n",
    "4. **LLM Generation**: RunPod vLLM endpoint with SatcomLLM model\n",
    "5. **RAG Orchestration**: Custom pipeline combining retrieval and generation\n",
    "\n",
    "\n",
    "### Next Steps:\n",
    "- Try different questions\n",
    "- Adjust top_k for more/less context\n",
    "- Experiment with temperature settings\n",
    "- Add more documents to the knowledge base\n",
    "- Implement re-ranking for better results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea1670",
   "metadata": {},
   "source": [
    "## Bonus: PDF to Markdown Conversion\n",
    "\n",
    "If you have PDF documents, here's a utility function to convert them to markdown for processing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cc9755d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF to Markdown converter ready!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "\n",
    "def pdf_to_markdown(pdf_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Convert PDF to markdown format.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        output_path: Optional path for markdown output\n",
    "    \n",
    "    Returns:\n",
    "        Markdown text content\n",
    "    \"\"\"\n",
    "    # Open PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    markdown_content = []\n",
    "    \n",
    "    # Extract text from each page\n",
    "    for page_num, page in enumerate(doc, 1):\n",
    "        # Extract text\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Add page marker\n",
    "        markdown_content.append(f\"\\n## Page {page_num}\\n\")\n",
    "        markdown_content.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    # Combine all content\n",
    "    full_markdown = \"\\n\".join(markdown_content)\n",
    "    \n",
    "    # Save to file if output path provided\n",
    "    if output_path:\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_markdown)\n",
    "        print(f\"Markdown saved to: {output_path}\")\n",
    "    \n",
    "    return full_markdown\n",
    "\n",
    "# Example usage:\n",
    "# pdf_path = \"./data/sample_document.pdf\"\n",
    "# markdown_text = pdf_to_markdown(pdf_path, \"./data/sample_document.md\")\n",
    "# # Then use chunk_markdown_document(markdown_text) to chunk it\n",
    "\n",
    "print(\"PDF to Markdown converter ready!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
