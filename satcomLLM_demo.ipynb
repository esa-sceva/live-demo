{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20d0a3c",
   "metadata": {},
   "source": [
    "# SatcomLLM: Live Chat & RAG Pipeline Demo\n",
    "\n",
    "Welcome to the SatcomLLM Live Demo! This interactive notebook teaches you how to build a complete, production-ready Retrieval-Augmented Generation (RAG) system for satellite communications domain knowledge.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook demonstrates how to build a RAG system that combines semantic search with large language models to create grounded, source-attributed responses. You'll learn to store documents in a vector database, retrieve relevant context based on user queries, and generate answers using a cloud-hosted LLM.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "The demo is divided into 4 main parts:\n",
    "\n",
    "- **Setup and LLM Fundamentals**: install dependencies, configure API keys, and understand core concepts like tokenization and prompting. You'll test the connection to the RunPod vLLM endpoint to ensure everything works before building the RAG system.\n",
    "\n",
    "- **Theory around RAG**: understand the principles of Retrieval-Augmented Generation (RAG), including how retrieval complements generative models by providing up-to-date and domain-specific context.\n",
    "\n",
    "- **Embedding Vector Database Creation**: load and chunk markdown documents, generate embeddings using DeepInfra, create a Qdrant vector database collection, and populate it with embedded document chunks. This builds the knowledge base that powers semantic search.\n",
    "\n",
    "- **Retrieval and RAG Pipeline**: implement semantic search to find relevant documents, build the complete RAG pipeline that combines retrieval with generation, and test the system with real questions. You'll see how retrieved context improves answer quality and enables source attribution.\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "This system uses DeepInfra for embeddings (2560-dimensional vectors from Qwen3-4B), Qdrant Cloud for vector storage and similarity search, and RunPod vLLM for fast inference with the SatcomLLM model. Documents are processed with LangChain's header-based chunking to preserve semantic structure.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You'll need Python 3.8 or higher and API keys for DeepInfra, RunPod, and Qdrant Cloud. The complete demo takes approximately 10 minutes to run.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049b5cc",
   "metadata": {},
   "source": [
    "# PART 1: Setup & LLM Fundamentals\n",
    "\n",
    "Before building our RAG system, let's set up the environment and understand key LLM concepts.\n",
    "\n",
    "## What's in This Section?\n",
    "\n",
    "### 1. Dependency Installation\n",
    "Install all required Python packages:\n",
    "- `transformers` & `torch`: LLM libraries\n",
    "- `langchain`: RAG orchestration  \n",
    "- `qdrant-client`: Vector database\n",
    "- `requests`: API calls\n",
    "- `python-dotenv`: Environment management\n",
    "\n",
    "### 2. API Configuration\n",
    "Set up three essential services:\n",
    "- **DeepInfra**: For generating embeddings\n",
    "- **RunPod**: For LLM inference with vLLM\n",
    "- **Qdrant Cloud**: For vector storage\n",
    "\n",
    "### 3. LLM Concepts\n",
    "Learn the fundamentals:\n",
    "- **Tokenization**: Text → Numbers\n",
    "- **Prompting**: Crafting instructions\n",
    "- **Generation**: Controlling output\n",
    "\n",
    "### 4. API Testing\n",
    "Verify your RunPod vLLM connection works before building RAG.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb0f9e-1b3e-4a8c-8e66-9c70e5baf2dc",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1. Setup Instructions\n",
    "\n",
    "\n",
    "First, install all required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24cd6824-878b-4c08-acfd-3f1f618c1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22281c87",
   "metadata": {},
   "source": [
    "### 1.2 Configure API Keys\n",
    "\n",
    "Copy `env.example` to `.env` and fill in your API keys.\n",
    "\n",
    "**Required API Keys:**\n",
    "\n",
    "1. **DeepInfra API Key** (for embeddings):\n",
    "   - Sign up at [deepinfra.com](https://deepinfra.com)\n",
    "   - Get your API key from the dashboard\n",
    "   - Free tier available with generous limits\n",
    "\n",
    "2. **RunPod API Key** (for LLM inference):\n",
    "   - Create account at [runpod.io](https://runpod.io)\n",
    "   - Contact [SatComLLM team](https://github.com/esa-sceva) for the endpoint URL and API key\n",
    "\n",
    "3. **Qdrant Cloud** (optional, for production):\n",
    "   - Sign up at [qdrant.tech](https://qdrant.tech)\n",
    "   - Create a cluster (free tier available)\n",
    "   - Get your cluster URL and API key\n",
    "   - *Note: The notebook will use in-memory storage if not configured*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5287d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are installed correctly\n"
     ]
    }
   ],
   "source": [
    "# Verify imports work correctly\n",
    "try:\n",
    "    import transformers\n",
    "    import langchain\n",
    "    import qdrant_client\n",
    "    import openai\n",
    "    print(\"All required packages are installed correctly\")\n",
    "except ImportError as e:\n",
    "    print(\"f Missing package: {e}\")\n",
    "    print(\"\\nPlease install dependencies:\")\n",
    "    print(\"  pip install -r requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e5984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Configuration Status:\n",
      "============================================================\n",
      "  DeepInfra (Embeddings)    Set\n",
      "  RunPod (LLM Inference)    Set\n",
      "  RunPod Endpoint URL       Set\n",
      "  Qdrant Cloud (Optional)   Set\n",
      "============================================================\n",
      "All required API keys are configured!\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if required API keys are set\n",
    "print(\"Environment Configuration Status:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Required keys\n",
    "required_keys = {\n",
    "    \"DEEPINFRA_API_KEY\": \"DeepInfra (Embeddings)\",\n",
    "    \"RUNPOD_API_KEY\": \"RunPod (LLM Inference)\",\n",
    "    \"RUNPOD_API_URL\": \"RunPod Endpoint URL\"\n",
    "}\n",
    "\n",
    "all_set = True\n",
    "for key, description in required_keys.items():\n",
    "    status = \"Set\" if os.getenv(key) else \"NOT SET (REQUIRED)\"\n",
    "    print(f\"  {description:25} {status}\")\n",
    "    if not os.getenv(key):\n",
    "        all_set = False\n",
    "\n",
    "# Optional keys\n",
    "print(f\"  {'Qdrant Cloud':25} {'Set' if os.getenv('QDRANT_URL') else '○ Not set (will use in-memory)'}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_set:\n",
    "    print(\"All required API keys are configured!\")\n",
    "else:\n",
    "    print(\"\\nMissing required API keys!\")\n",
    "    print(\"   Please copy env.example to .env and add your keys.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9ed42",
   "metadata": {},
   "source": [
    "## 1.3 Tokenization: Breaking Text into Tokens\n",
    "\n",
    "Before a language model can process text, the text must be converted into a form the model can understand. This is why **tokenization** is essential: it transforms raw text into numerical units that the model can operate on. Modern LLMs do not rely on full words as atomic units; instead, they use **subword tokenization**, which divides text into smaller, reusable components called *tokens*.\n",
    "\n",
    "### Why Subword Tokenization?\n",
    "\n",
    "Subword tokenization is used because it strikes a balance between word-level and character-level representations. In particular, it offers several practical advantages:\n",
    "\n",
    "* **Robust handling of rare or unseen words**: Words that do not appear in the training vocabulary can still be represented by combining known subword units.\n",
    "* **Smaller, more efficient vocabularies**: Keeping the vocabulary compact reduces memory requirements and speeds up training and inference.\n",
    "* **Better capture of linguistic structure**: Morphological patterns (such as prefixes, suffixes, and stems) are encoded naturally through shared subword units.\n",
    "\n",
    "For example, the word *\"satellite\"* might be tokenized into `[\"sat\", \"ell\", \"ite\"]`. These components can help the model generalize to related terms such as *\"satellites\"* or even domain-specific variations like *\"satcom\"*.\n",
    "\n",
    "Let's see tokenization in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097d8bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Satellite communications enable global connectivity through geostationary and low Earth orbit satellites.\n",
      "\n",
      "Number of tokens: 22\n",
      "\n",
      "Tokens: ['<s>', '▁Sat', 'ellite', '▁communic', 'ations', '▁enable', '▁global', '▁connect', 'ivity', '▁through', '▁ge', 'ost', 'ation', 'ary', '▁and', '▁low', '▁Earth', '▁orbit', '▁sat', 'ell', 'ites', '.']\n",
      "\n",
      "Token IDs: [1, 12178, 20911, 7212, 800, 9025, 5534, 4511, 2068, 1549, 1737, 520, 362, 653, 322, 4482, 11563, 16980, 3290, 514, 3246, 29889]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a popular tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')\n",
    "\n",
    "# Example text about satellite communications\n",
    "text = \"Satellite communications enable global connectivity through geostationary and low Earth orbit satellites.\"\n",
    "\n",
    "# Tokenize the text\n",
    "encoded = tokenizer(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0])\n",
    "\n",
    "print(f\"Original text: {text}\\n\")\n",
    "print(f\"Number of tokens: {len(tokens)}\\n\")\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "print(f\"Token IDs: {encoded['input_ids'][0].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef89b15",
   "metadata": {},
   "source": [
    "### Understanding Token Representation\n",
    "\n",
    "Notice that:\n",
    "\n",
    "* Some words become **single tokens** (especially common words).\n",
    "* Other words are **split into multiple subword tokens** (often technical terms or rare words).\n",
    "* **Special tokens** may appear (e.g., `<s>` for start-of-sequence, `</s>` for end-of-sequence).\n",
    "* Prefixes like `Ġ` or `▁` often indicate a **leading space** before the word in certain tokenizers.\n",
    "\n",
    "Tokenization affects several aspects of LLM usage:\n",
    "\n",
    "* **Context window**: each model has a maximum number of tokens it can process (e.g., 4096, 8192…).\n",
    "* **API costs**: most LLM APIs charge based on the number of tokens processed.\n",
    "* **Inference speed**: more tokens mean slower processing and higher latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f4747",
   "metadata": {},
   "source": [
    "## 1.4 Prompting: Instructing Language Models\n",
    "\n",
    "Prompting refers to the process of designing input instructions that guide a language model to produce the desired output. The way a prompt is structured can significantly influence the quality, relevance, and accuracy of the model’s responses. In essence, effective prompting bridges the gap between human intent and model understanding.\n",
    "\n",
    "### Why Prompting Matters\n",
    "\n",
    "* **Improves response quality**: Clear and specific prompts help the model generate accurate and relevant answers.\n",
    "* **Guides model behavior**: Prompts can specify tone, style, format, or constraints for the response.\n",
    "* **Enables few-shot learning**: Including examples in the prompt allows the model to learn patterns and apply them to new queries without retraining.\n",
    "\n",
    "### Prompt Structure in Modern Chat Models\n",
    "\n",
    "Most modern chat-oriented LLMs use a structured prompt format composed of multiple message types:\n",
    "\n",
    "1. **System Message**\n",
    "   Sets the model’s role, behavior, or overall context. For example, a system message might instruct the model to act as a tutor, an assistant, or a domain expert.\n",
    "\n",
    "2. **User Message**\n",
    "   Contains the main query, instruction, or request. This is the content that the model is expected to respond to.\n",
    "\n",
    "3. **Assistant Message**\n",
    "   Represents the model’s response, or can include few-shot examples demonstrating the expected behavior. These examples help the model generalize the instruction pattern to new queries.\n",
    "\n",
    "Each message is internally encoded with special tokens that help the model distinguish between system instructions, user queries, and assistant outputs. This structure allows the model to maintain coherent multi-turn conversations and follow complex instructions more effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c0115f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      "<|system|>\n",
      "You are a helpful assistant specializing in satellite communications and space technology. \n",
      "Provide accurate, technical information while remaining accessible to your audience.\n",
      "<|end|>\n",
      "<|user|>\n",
      "What is the difference between GEO and LEO satellites?\n",
      "<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of a well-structured prompt\n",
    "prompt_template = \"\"\"<|system|>\n",
    "You are a helpful assistant specializing in satellite communications and space technology. \n",
    "Provide accurate, technical information while remaining accessible to your audience.\n",
    "<|end|>\n",
    "<|user|>\n",
    "{question}\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is the difference between GEO and LEO satellites?\"\n",
    "formatted_prompt = prompt_template.format(question=question)\n",
    "\n",
    "print(\"Formatted Prompt:\")\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34712da",
   "metadata": {},
   "source": [
    "### Prompting Best Practices\n",
    "\n",
    "1. Be Specific: Clearly state what you want\n",
    "2. Provide Context: Give relevant background information\n",
    "3. Use Examples: Few-shot prompting improves accuracy\n",
    "4. Set Constraints: Specify format, length, or style requirements\n",
    "5. System Message: Use it to set expertise domain and behavior\n",
    "\n",
    "### Key parameters for LLM text generation:\n",
    "\n",
    "  - max_tokens: Maximum number of tokens to generate (e.g., 256, 512, 1024)\n",
    "  - temperature: Controls randomness (0.0 = deterministic, 1.0 = very creative)\n",
    "  - top_p: Nucleus sampling - considers top probability mass (e.g., 0.9 = top 90%)\n",
    "  - top_k: Only consider top K most likely tokens at each step\n",
    "\n",
    "Now let's see prompting in action with a real model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e015d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokenization example:\n",
      "\n",
      "Prompt: <|system|>\n",
      "You are a helpful assistant specializing in satellite communications and space technology...\n",
      "\n",
      "Number of tokens in prompt: 80\n",
      "First 20 token IDs: [1, 529, 29989, 5205, 29989, 29958, 13, 3492, 526, 263, 8444, 20255, 4266, 5281, 297, 28421, 7212, 800, 322, 2913]\n",
      "\n",
      "We'll use RunPod vLLM for actual text generation\n"
     ]
    }
   ],
   "source": [
    "# For this notebook, we'll use RunPod vLLM for all LLM inference\n",
    "# This avoids downloading large models locally and provides production-grade performance\n",
    "\n",
    "# Let's demonstrate how the prompt would be tokenized\n",
    "print(\"Prompt tokenization example:\\n\")\n",
    "\n",
    "# Tokenize our formatted prompt from the previous cell\n",
    "prompt_tokens = tokenizer.encode(formatted_prompt)\n",
    "print(f\"Prompt: {formatted_prompt[:100]}...\")\n",
    "print(f\"\\nNumber of tokens in prompt: {len(prompt_tokens)}\")\n",
    "print(f\"First 20 token IDs: {prompt_tokens[:20]}\")\n",
    "\n",
    "# This helps us understand:\n",
    "# - How much of the model's context window the prompt uses\n",
    "# - Why API costs are often measured in tokens\n",
    "# - How to optimize prompts to fit within context limits\n",
    "\n",
    "print(\"\\nWe'll use RunPod vLLM for actual text generation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7346e-4189-473c-8024-9f4084080676",
   "metadata": {},
   "source": [
    "## 1.5 Testing the RunPod vLLM API\n",
    "\n",
    "Before we proceed to RAG, it is important to ensure that our connection to the RunPod vLLM server is working correctly. This step verifies that the API endpoint is properly configured and that the SatcomLLM model hosted on the server is accessible.\n",
    "\n",
    "### RunPod vLLM Endpoint\n",
    "\n",
    "RunPod provides a scalable infrastructure for hosting large language models with low-latency inference. The vLLM API endpoint allows us to send text prompts to the hosted model and receive generated responses in real time. Testing this endpoint ensures that:\n",
    "\n",
    "* Network connectivity to the server is functional.\n",
    "* API authentication (if required) is correctly configured.\n",
    "* The hosted model is running and ready to accept requests.\n",
    "\n",
    "### Verifying the Hosted Model\n",
    "\n",
    "The model hosted on RunPod is **[LLaMA3-Satcom-8B](https://huggingface.co/esa-sceva/llama3-satcom-8b)**. By sending a test prompt to this endpoint, we can:\n",
    "\n",
    "* Confirm that the endpoint is reachable and responding correctly.\n",
    "* Ensure that the SatcomLLM model interprets queries as intended and generates coherent, domain-specific outputs.\n",
    "* Validate the setup before integrating the model into downstream workflows such as embeddings, retrieval, or RAG pipelines.\n",
    "\n",
    "Performing this test gives us confidence that the RunPod-hosted SatcomLLM is ready for production use and that our API integration is functioning properly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe8089-198b-4934-a655-1b4fba024e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunPod vLLM client initialized\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Get API credentials from environment\n",
    "API_URL = os.getenv(\"RUNPOD_API_URL\")\n",
    "API_KEY = os.getenv(\"RUNPOD_API_KEY\")\n",
    "\n",
    "def poll_job_status(job_id, max_attempts=30, delay=2):\n",
    "    \"\"\"Poll RunPod job status until completion.\"\"\"\n",
    "    status_url = f\"{API_URL.rsplit('/', 1)[0]}/status/{job_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        response = requests.get(status_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            status = result.get(\"status\")\n",
    "            \n",
    "            if status == \"COMPLETED\":\n",
    "                output = result.get(\"output\")\n",
    "                \n",
    "                # Parse vLLM response structure\n",
    "                if isinstance(output, list) and len(output) > 0:\n",
    "                    if \"choices\" in output[0]:\n",
    "                        choices = output[0][\"choices\"]\n",
    "                        if len(choices) > 0:\n",
    "                            # Try tokens array first (vLLM format)\n",
    "                            if \"tokens\" in choices[0]:\n",
    "                                return \"\".join(choices[0][\"tokens\"])\n",
    "                            # Try message format (OpenAI compatible)\n",
    "                            elif \"message\" in choices[0]:\n",
    "                                return choices[0][\"message\"][\"content\"]\n",
    "                \n",
    "                # OpenAI-style response\n",
    "                if isinstance(output, dict) and \"choices\" in output:\n",
    "                    return output[\"choices\"][0][\"message\"][\"content\"]\n",
    "                \n",
    "                # Direct string output\n",
    "                if isinstance(output, str):\n",
    "                    return output\n",
    "                    \n",
    "                return str(output)\n",
    "                \n",
    "            elif status == \"FAILED\":\n",
    "                return f\"Job failed: {result.get('error', 'Unknown error')}\"\n",
    "            \n",
    "            # Still running, wait and retry\n",
    "            time.sleep(delay)\n",
    "        else:\n",
    "            return f\"Status check error {response.status_code}: {response.text}\"\n",
    "    \n",
    "    return \"Timeout waiting for job completion\"\n",
    "\n",
    "def chat_with_vllm(messages, max_tokens=512, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Send chat request to RunPod vLLM server.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dicts with 'role' and 'content'\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0 = deterministic)\n",
    "        \n",
    "    Returns:\n",
    "        Generated text response\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"max_new_tokens\": max_tokens,  # Alternative parameter name\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        \n",
    "        # RunPod returns a job ID for async execution\n",
    "        if \"id\" in result:\n",
    "            job_id = result[\"id\"]\n",
    "            print(f\"Job submitted: {job_id}\")\n",
    "            print(\"Waiting for response...\")\n",
    "            return poll_job_status(job_id)\n",
    "        \n",
    "        # Direct response (synchronous mode)\n",
    "        if \"output\" in result:\n",
    "            return result[\"output\"]\n",
    "    \n",
    "    return f\"Error {response.status_code}: {response.text}\"\n",
    "\n",
    "print(\"RunPod vLLM client initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca6bce15-eafc-4351-b467-8f26f86d3aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RunPod vLLM API...\n",
      "Question: What is satellite communications in one sentence?\n",
      "\n",
      "Job submitted: c5a09236-55d3-458f-9824-98b0c944f0db-e1\n",
      "Waiting for response...\n",
      "\n",
      "Response: Satellite communications involve the transmission of information, such as voice, data, and video, through space using satellites orbiting the Earth that act as repeaters to amplify and transmit signals to remote or distant locations, often providing global coverage and connectivity.\n",
      "\n",
      "RunPod vLLM API test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test the RunPod vLLM API with a sample question\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is satellite communications in one sentence?\"}\n",
    "]\n",
    "\n",
    "print(\"Testing RunPod vLLM API...\")\n",
    "print(f\"Question: {test_messages[0]['content']}\\n\")\n",
    "\n",
    "response = chat_with_vllm(test_messages, max_tokens=256, temperature=0.7)\n",
    "\n",
    "print(f\"\\nResponse: {response}\")\n",
    "print(\"\\nRunPod vLLM API test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15590cbb-fc6b-422d-8176-a0435ae96b27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PART 2: THEORY AROUND RAG SYSTEM\n",
    "\n",
    "Now that we know that the model is working on the RunPod endpoint and can answer questions, let's understand how to implement RAG, starting from **custom documents**\n",
    "But before we build, let's understand why and how use RAG.\n",
    "\n",
    "## 2.1. The Problem with Standard LLMs\n",
    "\n",
    "Large Language Models have limitations:\n",
    "\n",
    "| Issue | Description | Impact |\n",
    "|-------|-------------|--------|\n",
    "| **Hallucinations** | Generate plausible but false info | Unreliable answers |\n",
    "| **Knowledge Cutoff** | Training data has a date limit | Outdated information |\n",
    "| **No Source** | Can't cite where info came from | Unverifiable |\n",
    "| **Generic** | Lack domain-specific expertise | Poor specialized answers |\n",
    "| **Static** | Can't update without retraining | Expensive to maintain |\n",
    "\n",
    "## How RAG Solves These Problems\n",
    "\n",
    "**Retrieval-Augmented Generation** adds a knowledge retrieval step:\n",
    "\n",
    "```\n",
    "Standard LLM:\n",
    "Question → LLM → Answer (may hallucinate)\n",
    "\n",
    "RAG System:\n",
    "Question → Find Relevant Docs → LLM + Context → Grounded Answer\n",
    "```\n",
    "\n",
    "### Key Benefits:\n",
    "\n",
    "- Grounded: Answers based on actual documents  \n",
    "- Verifiable: Shows sources used  \n",
    "- Up-to-date: Update docs without retraining model  \n",
    "- Domain-specific: Add specialized knowledge  \n",
    "- Cost-effective: Cheaper than fine-tuning  \n",
    "\n",
    "## 2.2 RAG Architecture\n",
    "\n",
    "### Two Main Phases:\n",
    "\n",
    "#### Phase 1: Indexing (One-Time Setup)\n",
    "```\n",
    "Documents → Clean → Chunk → Embed → Store in Vector DB\n",
    "```\n",
    "This is what we'll do in this part!\n",
    "\n",
    "#### Phase 2: Query (Every Question)\n",
    "```\n",
    "Question → Embed → Search Vector DB → Retrieve Docs → \n",
    "    Augment Prompt → LLM → Answer\n",
    "```\n",
    "This is what we will implement in part 3!\n",
    "\n",
    "## 2.3 When to Use RAG\n",
    "\n",
    "| Use Case | RAG? | Why |\n",
    "|----------|------|-----|\n",
    "| Documentation Q&A | Yes | Need exact info from docs |\n",
    "| Technical support | Yes | Must cite solutions |\n",
    "| General chat | No | Model knowledge sufficient |\n",
    "| Research queries | Yes | Need to reference papers |\n",
    "| Identity info | No | Model can hallucinate |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e45f75",
   "metadata": {},
   "source": [
    "\n",
    "## 2.4 What We'll Build\n",
    "\n",
    "In the following cells, we will:\n",
    "\n",
    "1. Load and Chunk Documents - Process satcom docs into semantic chunks\n",
    "2. Set Up Embeddings - Embed the chunks for retrieving tasks, before populating the vector database.  \n",
    "3. Create Vector Database on Qdrant\n",
    "4. Upload Knowledge Base - Store all document chunks in the vector DB\n",
    "5. Implement Search - Semantic retrieval from the vector database\n",
    "6. Build RAG Pipeline - Combine retrieval with vLLM generation\n",
    "7. Test & Interact - Ask questions and get grounded answers\n",
    "\n",
    "## 2.5 Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────┐\n",
    "│   User      │ Asks a question\n",
    "│  Question   │\n",
    "└──────┬──────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  1. EMBED QUERY (DeepInfra)              │\n",
    "│     Convert question → 2560-dim vector   │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  2. SEMANTIC SEARCH (Qdrant Cloud)       │\n",
    "│     Find top-k similar document chunks   │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  3. RETRIEVE CONTEXT                     │\n",
    "│     Get text + metadata from matches     │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  4. ENRICH PROMPT                        │\n",
    "│     Question + Retrieved Context         │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────────────────────────────────┐\n",
    "│  5. GENERATE ANSWER (RunPod vLLM)        │\n",
    "│     SatcomLLM produces grounded response │\n",
    "└──────┬───────────────────────────────────┘\n",
    "       ↓\n",
    "┌──────────────┐\n",
    "│   Answer     │ With source attribution\n",
    "│ + Sources    │\n",
    "└──────────────┘\n",
    "```\n",
    "\n",
    "\n",
    "Let's start building!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bba21",
   "metadata": {},
   "source": [
    "# Part 3: Embedding & Vector Database\n",
    "\n",
    "In a Retrieval-Augmented Generation (RAG) system, as we have seen before, the knowledge base is represented in a way that allows fast and semantically meaningful search. This is achieved by **embedding** text into high-dimensional vectors and storing them in a **vector database**.\n",
    "\n",
    "In this part of the pipeline, we will:\n",
    "\n",
    "1. **Load documents** from local files (Markdown or converted PDFs).\n",
    "2. **Chunk the documents** intelligently to preserve semantic and hierarchical structure.\n",
    "3. **Generate embeddings** for each chunk using a specialized embedding model.\n",
    "4. **Upload the embeddings to a vector database**, making them ready for retrieval during RAG.\n",
    "\n",
    "\n",
    "\n",
    "## 3.1. Document Loading and Chunking\n",
    "\n",
    "Before we can build our RAG system, we need to prepare our knowledge base. This involves:\n",
    "\n",
    "### Document Loading\n",
    "We'll use a couple of Markdown files in the `data` folder as our example of knowledge source, any other kind of `.md` file is fine. We will also explore how to easily convert `pdfs` into `.md` using a simple python library if needed.\n",
    "\n",
    "### Intelligent Chunking\n",
    "Then we use header-based markdown splitting (hierarchical chunking) to preserve semantic structure:\n",
    "- Split on markdown headers (#, ##, ###, ####)\n",
    "- Maintain parent header context in metadata\n",
    "- Respect maximum chunk size (1000 characters)\n",
    "- Add overlap between chunks for continuity\n",
    "\n",
    "This approach is inspired by the SatcomLLM pipeline's own chunking strategies and ensures:\n",
    "- Semantic coherence - Chunks respect document structure\n",
    "- Context preservation - Headers provide hierarchical context  \n",
    "- Optimal retrieval - Chunks are sized for effective embedding\n",
    "\n",
    "**Run the next 2 cells to load and chunk the document:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a2d098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 markdown file(s) in 'data':\n",
      "  - README.md\n",
      "  - README_1.md\n",
      "\n",
      "README.md: 7871 characters\n",
      "\n",
      "README_1.md: 8498 characters\n",
      "\n",
      "Total characters across all documents: 16369\n",
      "\n",
      "Preview of first document:\n",
      "# Synthetic QA Generation for ESA Satcom LLM\n",
      "\n",
      "This repository contains a modular and customizable pipeline to generate high-quality Question & Answer (QA) pairs from markdown documents. The purpose of this pipeline is to create synthetic training data for fine-tuning or evaluating language models in the satellite communications domain.\n",
      "\n",
      "---\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "- [Pipeline Overview](#pipeline-overview)\n",
      "- [Repository Structure](#repository-structure)\n",
      "- [Quick Start](#quick-start)\n",
      "- [QA Generati\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all markdown documents from the data folder\n",
    "data_folder = Path('data')\n",
    "\n",
    "# Check if data folder exists\n",
    "if not data_folder.exists():\n",
    "    raise FileNotFoundError(f\"Data folder not found: {data_folder}\")\n",
    "\n",
    "# Find all markdown files\n",
    "markdown_files = list(data_folder.glob('*.md')) + list(data_folder.glob('*.markdown'))\n",
    "\n",
    "if not markdown_files:\n",
    "    raise FileNotFoundError(f\"No markdown files found in {data_folder}\")\n",
    "\n",
    "print(f\"Found {len(markdown_files)} markdown file(s) in '{data_folder}':\")\n",
    "for f in markdown_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# Load all documents\n",
    "documents = {}\n",
    "total_chars = 0\n",
    "\n",
    "for file_path in markdown_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        documents[file_path.name] = content\n",
    "        total_chars += len(content)\n",
    "        print(f\"\\n{file_path.name}: {len(content)} characters\")\n",
    "\n",
    "print(f\"\\nTotal characters across all documents: {total_chars}\")\n",
    "print(f\"\\nPreview of first document:\")\n",
    "first_doc = list(documents.values())[0]\n",
    "print(first_doc[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f8bab",
   "metadata": {},
   "source": [
    "We'll use LangChain's `MarkdownHeaderTextSplitter` which aligns with the SatcomLLM chunking strategy from `synthetic_gen/chunker/`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "896bdcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing README.md...\n",
      "  Generated 19 chunks\n",
      "\n",
      "Processing README_1.md...\n",
      "  Generated 28 chunks\n",
      "\n",
      "============================================================\n",
      "Total chunks across all documents: 47\n",
      "============================================================\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Source: README.md\n",
      "Headers: Synthetic QA Generation for ESA Satcom LLM > \n",
      "Content preview: This repository contains a modular and customizable pipeline to generate high-quality Question & Answer (QA) pairs from markdown documents. The purpos...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Source: README.md\n",
      "Headers: Synthetic QA Generation for ESA Satcom LLM > Table of Contents\n",
      "Content preview: - [Pipeline Overview](#pipeline-overview)\n",
      "- [Repository Structure](#repository-structure)\n",
      "- [Quick Start](#quick-start)\n",
      "- [QA Generation Strategies](#...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Source: README.md\n",
      "Headers: Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview\n",
      "Content preview: > **Official Datasets**: The official SatcomLLM synthetic QA datasets ([esa-sceva/satcom-synth-qa](https://huggingface.co/datasets/esa-sceva/satcom-sy...\n"
     ]
    }
   ],
   "source": [
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "\n",
    "for filename, markdown_text in documents.items():\n",
    "    print(f\"\\nProcessing {filename}...\")\n",
    "    doc_chunks = chunk_markdown_document(markdown_text)\n",
    "    \n",
    "    # Add source filename to metadata\n",
    "    for chunk in doc_chunks:\n",
    "        chunk.metadata['source_file'] = filename\n",
    "    \n",
    "    all_chunks.extend(doc_chunks)\n",
    "    print(f\"  Generated {len(doc_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total chunks across all documents: {len(all_chunks)}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show sample chunks from different documents\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(all_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Source: {chunk.metadata.get('source_file', 'unknown')}\")\n",
    "    print(f\"Headers: {chunk.metadata.get('Header 1', '')} > {chunk.metadata.get('Header 2', '')}\")\n",
    "    print(f\"Content preview: {chunk.page_content[:150]}...\")\n",
    "\n",
    "# Rename for consistency with rest of notebook\n",
    "chunks = all_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04ffc7",
   "metadata": {},
   "source": [
    "## 3.2 Initialize DeepInfra Embeddings\n",
    "\n",
    "Now we'll set up our embedding model using **DeepInfra's API**.\n",
    "\n",
    "### What are Embeddings?\n",
    "Embeddings convert text into dense numerical vectors that capture semantic meaning:\n",
    "- Similar texts → Similar vectors\n",
    "- Enable semantic search (not just keyword matching)\n",
    "- Foundation for RAG retrieval\n",
    "\n",
    "### Model Specifications\n",
    "- **Model**: `Qwen/Qwen3-Embedding-4B` \n",
    "- **Dimensions**: 2560\n",
    "- **Use Case**: High-quality semantic search and retrieval\n",
    "- **Normalization**: Vectors are normalized for cosine similarity\n",
    "\n",
    "Run the next cell to initialize the embedding model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DeepInfra embeddings with corrected settings...\n",
      "✓ DeepInfra embeddings initialized successfully!\n",
      "✓ Model: Qwen/Qwen3-Embedding-4B\n",
      "✓ Embedding dimension: 2560\n",
      "✓ Sample embedding (first 10 values): [-0.0002994537353515625, 0.0308837890625, -0.0194091796875, 0.016357421875, -0.00115966796875, 0.058349609375, 0.060302734375, 0.000591278076171875, 0.023193359375, -0.006500244140625]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Fixed DeepInfra Embeddings API wrapper\n",
    "class DeepInfraEmbeddings:\n",
    "    \"\"\"Embeddings using DeepInfra API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, model=\"Qwen/Qwen3-Embedding-4B\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.api_url = \"https://api.deepinfra.com/v1/openai/embeddings\"  # Fixed URL\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embed a list of documents\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"input\": texts\n",
    "        }\n",
    "        response = requests.post(self.api_url, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return [item[\"embedding\"] for item in result[\"data\"]]\n",
    "        else:\n",
    "            raise Exception(f\"DeepInfra API error: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embed a single query\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# Initialize DeepInfra embedding model with FIXED settings\n",
    "print(\"Initializing DeepInfra embeddings with corrected settings...\")\n",
    "embedding_model = DeepInfraEmbeddings(\n",
    "    api_key=os.getenv(\"DEEPINFRA_API_KEY\"),\n",
    "    model=\"Qwen/Qwen3-Embedding-4B\"  # Known working model, 1024 dimensions\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "sample_text = \"The SatcomLLM pipeline generates synthetic QA pairs from documents\"\n",
    "sample_embedding = embedding_model.embed_query(sample_text)\n",
    "\n",
    "print(f\"✓ DeepInfra embeddings initialized successfully!\")\n",
    "print(f\"✓ Model: {embedding_model.model}\")\n",
    "print(f\"✓ Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"✓ Sample embedding (first 10 values): {sample_embedding[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330a2fc",
   "metadata": {},
   "source": [
    "## 3.3 Create Qdrant Vector Database Collection\n",
    "\n",
    "Time to set up our **vector database** where we'll store all document embeddings for fast retrieval.\n",
    "\n",
    "### What is Qdrant?\n",
    "**Qdrant** is a high-performance vector database designed for:\n",
    "- **Similarity Search**: Find semantically similar documents in milliseconds\n",
    "- **Scalability**: Handle millions of vectors efficiently\n",
    "- **Filtering**: Combine vector search with metadata filters\n",
    "- **Production-Ready**: Built for real-world applications\n",
    "\n",
    "### Collection Configuration\n",
    "\n",
    "We're creating a collection with these specifications:\n",
    "\n",
    "| Parameter | Value | Explanation |\n",
    "|-----------|-------|-------------|\n",
    "| **Name** | `satcom_rag_demo` | Identifier for our knowledge base |\n",
    "| **Vector Size** | 2560 | Matches Qwen3-Embedding-4B output |\n",
    "| **Distance Metric** | Cosine | Best for normalized embeddings |\n",
    "| **Storage** | Qdrant Cloud | Persistent, managed storage |\n",
    "\n",
    "\n",
    "Run the next cell to create the collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4b8f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Qdrant Cloud...\n",
      "Creating collection: satcom_rag_demo\n",
      "✓ Collection 'satcom_rag_demo' created successfully!\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "\n",
    "# Initialize Qdrant Cloud client\n",
    "print(\"Connecting to Qdrant Cloud...\")\n",
    "qdrant_client = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_URL\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "# Collection name\n",
    "collection_name = \"satcom_rag_demo\"\n",
    "\n",
    "# Create new collection\n",
    "print(f\"Creating collection: {collection_name}\")\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=2560, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "print(f\"✓ Collection '{collection_name}' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d24f04",
   "metadata": {},
   "source": [
    "And upload chunks created to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "073883f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and uploading 47 chunks to Qdrant...\n",
      "This may take a moment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedding_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m texts \u001b[38;5;241m=\u001b[39m [chunk\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m batch_chunks]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Generate embeddings for batch\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create points\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, (chunk, embedding) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(batch_chunks, embeddings)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_model' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Upload chunks to Qdrant with embeddings\n",
    "print(f\"Embedding and uploading {len(chunks)} chunks to Qdrant...\")\n",
    "print(\"This may take a moment...\")\n",
    "\n",
    "# Prepare points for batch upload\n",
    "points = []\n",
    "\n",
    "# Process chunks in batches to avoid API rate limits\n",
    "batch_size = 10\n",
    "for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing chunks\"):\n",
    "    batch_chunks = chunks[i:i+batch_size]\n",
    "    \n",
    "    # Get texts from batch\n",
    "    texts = [chunk.page_content for chunk in batch_chunks]\n",
    "    \n",
    "    # Generate embeddings for batch\n",
    "    embeddings = embedding_model.embed_documents(texts)\n",
    "    \n",
    "    # Create points\n",
    "    for j, (chunk, embedding) in enumerate(zip(batch_chunks, embeddings)):\n",
    "        point_id = str(uuid.uuid4())\n",
    "        points.append(\n",
    "            PointStruct(\n",
    "                id=point_id,\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"text\": chunk.page_content,\n",
    "                    \"metadata\": chunk.metadata,\n",
    "                    \"chunk_id\": chunk.metadata.get(\"chunk_id\", f\"{i+j}\"),\n",
    "                    \"source\": chunk.metadata.get(\"source_file\")\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Upload all points to Qdrant\n",
    "print(f\"\\nUploading {len(points)} points to Qdrant...\")\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(f\"✓ Successfully uploaded {len(points)} chunks to Qdrant!\")\n",
    "\n",
    "# Verify collection info\n",
    "collection_info = qdrant_client.get_collection(collection_name=collection_name)\n",
    "print(f\"\\nCollection info:\")\n",
    "print(f\"  - Points count: {collection_info.points_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806cd55",
   "metadata": {},
   "source": [
    "# Part 4: Retrieval, Prompt Enrichment, and Generation\n",
    "\n",
    "In this part, we move from preparing the knowledge base to actively using it in a RAG workflow. The goal is to answer user queries by:\n",
    "\n",
    "1. Retrieving relevant document chunks from the vector database using semantic similarity search.\n",
    "2. Enriching the prompt by combining the retrieved context with the user query, ensuring the model has all the necessary information.\n",
    "3. Generating responses with the RunPod vLLM-hosted SatcomLLM model, producing accurate and context-aware answers.\n",
    "\n",
    "\n",
    "This step effectively demonstrates the full RAG loop: from query to retrieval, prompt construction, and finally, high-quality answer generation using a domain-specific LLM. By combining semantic search with prompt engineering, we ensure that even complex or technical queries are answered accurately and coherently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfcf4c",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1: Implement Semantic Search\n",
    "\n",
    "With our knowledge base populated, first let's build the **search function** that retrieves the most relevant document chunks for any user query.\n",
    "\n",
    "#### How Semantic Search Works\n",
    "\n",
    "```\n",
    "User Question\n",
    "    ↓\n",
    "1. Embed question using DeepInfra\n",
    "    ↓\n",
    "2. Query Qdrant with question vector\n",
    "    ↓\n",
    "3. Qdrant finds similar document vectors\n",
    "    ↓\n",
    "4. Return top-k most similar chunks\n",
    "    ↓\n",
    "Retrieved Documents + Similarity Scores\n",
    "```\n",
    "\n",
    "#### Understanding Similarity Scores\n",
    "\n",
    "The **cosine similarity score** ranges from 0 to 1:\n",
    "- **0.9 - 1.0**: Extremely similar (near-duplicate)\n",
    "- **0.7 - 0.9**: Highly relevant (strong match)\n",
    "- **0.5 - 0.7**: Moderately relevant (partial match)\n",
    "- **< 0.5**: Weakly relevant (consider filtering out)\n",
    "\n",
    "#### Function Features\n",
    "- **Top-K Retrieval**: Get the k most similar documents\n",
    "- **Score Transparency**: See exactly how relevant each result is\n",
    "- **Metadata Preservation**: Returns headers and structure info\n",
    "- **Fast**: Typically < 100ms for searches\n",
    "\n",
    "Run the next cell to implement and test the search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4828427a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Query: How does the QA generation pipeline work?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Result 1 (Score: 0.7946) ---\n",
      "Section: Synthetic QA Generation for ESA Satcom LLM > Table of Contents\n",
      "Text preview: - [Pipeline Overview](#pipeline-overview)\n",
      "- [Repository Structure](#repository-structure)\n",
      "- [Quick Start](#quick-start)\n",
      "- [QA Generation Strategies](#qa-generation-strategies)\n",
      "- [Complete Pipeline Example](#complete-pipeline-example)\n",
      "- [Next Steps](#next-steps)  \n",
      "---...\n",
      "\n",
      "--- Result 2 (Score: 0.7365) ---\n",
      "Section: Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview\n",
      "Text preview: Score QA pairs from 1-5 on four aspects using an LLM-as-a-judge to filter out low-quality pairs.  \n",
      "---...\n",
      "\n",
      "--- Result 3 (Score: 0.7125) ---\n",
      "Section: Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview\n",
      "Text preview: Generate QA pairs from the high-quality chunks using LLMs. Two strategies are available:\n",
      "- **Strategy 1 (All-in-One)**: Generate Q&A pairs together in a single pass\n",
      "- **Strategy 2 (Separate)**: Generate questions first, then answers separately...\n",
      "\n",
      "================================================================================\n",
      "✓ Search function working correctly!\n"
     ]
    }
   ],
   "source": [
    "def search_knowledge_base(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search the knowledge base for relevant documents.\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant documents with scores\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    \n",
    "    # Search in Qdrant using the correct API method\n",
    "    search_results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for result in search_results.points:\n",
    "        results.append({\n",
    "            \"text\": result.payload[\"text\"],\n",
    "            \"metadata\": result.payload[\"metadata\"],\n",
    "            \"score\": result.score\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the search function\n",
    "test_query = \"How does the QA generation pipeline work?\"\n",
    "print(f\"Test Query: {test_query}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = search_knowledge_base(test_query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} (Score: {result['score']:.4f}) ---\")\n",
    "    print(f\"Section: {result['metadata'].get('Header 1', '')} > {result['metadata'].get('Header 2', '')}\")\n",
    "    print(f\"Text preview: {result['text'][:300]}...\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Search function working correctly!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5bceb",
   "metadata": {},
   "source": [
    "## 4.2 Complete RAG Pipeline\n",
    "\n",
    "Now we bring everything together! This is the **main RAG function** that combines retrieval with generation.\n",
    "\n",
    "#### The Complete RAG Workflow\n",
    "\n",
    "#### Phase 1: Retrieval\n",
    "```\n",
    "Question → Embed → Search Qdrant → Top-K Documents\n",
    "```\n",
    "- Convert user question to vector\n",
    "- Find most similar chunks in vector database  \n",
    "- Retrieve actual text content and metadata\n",
    "\n",
    "#### Phase 2: Context Building\n",
    "```\n",
    "Retrieved Docs → Format with Headers → Enriched Context\n",
    "```\n",
    "- Format each document with its section headers\n",
    "- Include document numbers for reference\n",
    "- Combine into structured context string\n",
    "\n",
    "#### Phase 3: Prompt Engineering\n",
    "```\n",
    "System Prompt + Context + Question → Enriched Prompt\n",
    "```\n",
    "- Instruct the model to use provided context\n",
    "- Include all retrieved documents as context\n",
    "- Add the user's original question\n",
    "- Set clear expectations for the response\n",
    "\n",
    "#### Phase 4: Generation\n",
    "```\n",
    "Enriched Prompt → RunPod vLLM → Grounded Answer\n",
    "```\n",
    "- Send to SatcomLLM model on RunPod\n",
    "- vLLM provides fast, optimized inference\n",
    "- Model generates answer based on context\n",
    "- Returns answer with source attribution\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|---------|\n",
    "| **Grounded** | Answers based on actual documents, not hallucinations |\n",
    "| **Traceable** | Shows which documents were used |\n",
    "| **Transparent** | Displays similarity scores for trust |\n",
    "| **Configurable** | Adjust top_k, temperature, max_tokens |\n",
    "| **Fast** | vLLM optimization for quick responses |\n",
    "\n",
    "### Function Parameters\n",
    "\n",
    "- `question`: Your query (string)\n",
    "- `top_k`: Number of documents to retrieve (default: 3)\n",
    "- `max_tokens`: Maximum answer length (default: 512)\n",
    "- `temperature`: Creativity (0.0 = factual, 1.0 = creative)\n",
    "- `show_sources`: Display retrieved documents (default: True)\n",
    "\n",
    "Run the next cell to implement the complete RAG pipeline and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59b28e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Testing Complete RAG Pipeline\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question: What are the main stages of the SatcomLLM synthetic QA generation pipeline?\n",
      "================================================================================\n",
      "\n",
      "🔍 Searching knowledge base...\n",
      "\n",
      "📚 Retrieved 3 relevant documents:\n",
      "  [1] Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview (score: 0.773)\n",
      "  [2] Synthetic QA Generation for ESA Satcom LLM > Official Datasets (score: 0.753)\n",
      "  [3] Synthetic QA Generation for ESA Satcom LLM (score: 0.740)\n",
      "\n",
      "🤖 Generating answer with SatcomLLM...\n",
      "Job submitted: 19022a46-a5ed-406a-9a39-49b0857753b7-e2\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "According to the context, the main stages of the SatcomLLM synthetic QA generation pipeline consist of 4 stages as follows (from Document 1: Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview):\n",
      "\n",
      "1.  (No explicit stage description available in the provided context)\n",
      "2.  (No explicit stage description available in the provided context)\n",
      "3.  (No explicit stage description available in the provided context)\n",
      "4.  (No explicit stage description unfortunately not provided in the furnished\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the context, the main stages of the SatcomLLM synthetic QA generation pipeline consist of 4 stages as follows (from Document 1: Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview):\\n\\n1.  (No explicit stage description available in the provided context)\\n2.  (No explicit stage description available in the provided context)\\n3.  (No explicit stage description available in the provided context)\\n4.  (No explicit stage description unfortunately not provided in the furnished'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_rag_question(question, top_k=3, max_tokens=512, temperature=0.1, show_sources=True):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieve relevant documents and generate answer using vLLM.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        top_k: Number of documents to retrieve\n",
    "        max_tokens: Maximum tokens for generation\n",
    "        temperature: Sampling temperature\n",
    "        show_sources: Whether to display retrieved sources\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    print(\"Searching knowledge base...\")\n",
    "    retrieved_docs = search_knowledge_base(question, top_k=top_k)\n",
    "    \n",
    "    if show_sources:\n",
    "        print(f\"\\nRetrieved {len(retrieved_docs)} relevant documents:\")\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            headers = doc['metadata'].get('Header 1', '')\n",
    "            if doc['metadata'].get('Header 2'):\n",
    "                headers += f\" > {doc['metadata'].get('Header 2')}\"\n",
    "            print(f\"  [{i}] {headers} (score: {doc['score']:.3f})\")\n",
    "    \n",
    "    # Step 2: Format context from retrieved documents\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        headers = []\n",
    "        if doc['metadata'].get('Header 1'):\n",
    "            headers.append(doc['metadata']['Header 1'])\n",
    "        if doc['metadata'].get('Header 2'):\n",
    "            headers.append(doc['metadata']['Header 2'])\n",
    "        \n",
    "        section_info = \" > \".join(headers) if headers else \"General\"\n",
    "        context_parts.append(f\"[Document {i}] {section_info}\\n{doc['text']}\\n\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Step 3: Build enriched prompt\n",
    "    enriched_prompt = f\"\"\"You are a helpful assistant specializing in satellite communications and LLM pipelines.\n",
    "\n",
    "Use the following context from the documentation to answer the question accurately and concisely.\n",
    "If the answer cannot be found in the context, say so clearly.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer using vLLM\n",
    "    print(\"\\nGenerating answer with SatcomLLM...\")\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": enriched_prompt}\n",
    "    ]\n",
    "    \n",
    "    answer = chat_with_vllm(messages, max_tokens=max_tokens, temperature=temperature)\n",
    "    \n",
    "    # Display answer\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\"ANSWER:\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    print(answer)\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the complete RAG pipeline\n",
    "print(\"Testing Complete RAG Pipeline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test question 1\n",
    "ask_rag_question(\n",
    "    \"What are the main stages of the SatcomLLM synthetic QA generation pipeline?\",\n",
    "    top_k=3,\n",
    "    temperature=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734187a9",
   "metadata": {},
   "source": [
    "## 4.3 Testing with custom questions\n",
    "\n",
    "Let's test our RAG system with several diverse questions about the SatcomLLM pipeline to demonstrate its capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03a6b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Question: How does the chunking process work in the SatcomLLM pipeline?\n",
      "================================================================================\n",
      "\n",
      "🔍 Searching knowledge base...\n",
      "\n",
      "📚 Retrieved 3 relevant documents:\n",
      "  [1] Synthetic QA Generation for ESA Satcom LLM > Contributing (score: 0.657)\n",
      "  [2] Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview (score: 0.642)\n",
      "  [3] Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview (score: 0.605)\n",
      "\n",
      "🤖 Generating answer with SatcomLLM...\n",
      "Job submitted: a681ef94-d74e-42a2-b1fb-c68c80c9009b-e2\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Based on the provided context, the chunking process in the Satcom LLM pipeline is as follows:\n",
      "\n",
      "The pipeline splits raw markdown files into smaller, manageable chunks based on logical sections or size constraints. Chunks can be sourced from either local storage or an S3 storage, and splitting is done for file types .md or .mmd.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question: What are the two QA generation strategies and how do they differ?\n",
      "================================================================================\n",
      "\n",
      "🔍 Searching knowledge base...\n",
      "\n",
      "📚 Retrieved 3 relevant documents:\n",
      "  [1] Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview (score: 0.763)\n",
      "  [2] Synthetic QA Generation for ESA Satcom LLM > QA Generation Strategies (score: 0.725)\n",
      "  [3] Synthetic QA Generation for ESA Satcom LLM > Table of Contents (score: 0.707)\n",
      "\n",
      "🤖 Generating answer with SatcomLLM...\n",
      "Job submitted: 462f2890-8696-4963-adc4-ea7201c353b4-e2\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "There are two QA generation strategies:\n",
      "\n",
      "1. **Strategy 1 (All-in-One)**: Generates Q&A pairs together in a single pass, resulting in 'Good' quality at 'Fast' speed. Suitable for large-scale generation and rapid prototyping.\n",
      "\n",
      "2. **Strategy 2 (Separate)**: First generates questions, then answers separately, achieving 'Excellent' quality at a 'Slower' speed. Ideal for producing high-quality datasets with diverse questions.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question: How are low-quality QA pairs filtered in the pipeline?\n",
      "================================================================================\n",
      "\n",
      "🔍 Searching knowledge base...\n",
      "\n",
      "📚 Retrieved 3 relevant documents:\n",
      "  [1] Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview (score: 0.818)\n",
      "  [2] Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview (score: 0.660)\n",
      "  [3] Synthetic QA Generation for ESA Satcom LLM > Complete Pipeline Example (score: 0.630)\n",
      "\n",
      "🤖 Generating answer with SatcomLLM...\n",
      "Job submitted: cad36616-d48f-41cf-b7c7-a725f87e3757-e1\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Low-quality QA pairs are filtered using two methods:\n",
      "1. **Threshold filtering**: Chunks below a specified quality threshold are discarded.  \n",
      "2. **Percentile ranking**: Only the top P% percentile of high-quality chunks are kept (in \"2. Keep only the top p-percentile of high-quality chunks\" from Document 2).\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with more questions\n",
    "test_questions = [\n",
    "    \"How does the chunking process work in the SatcomLLM pipeline?\",\n",
    "    \"What are the two QA generation strategies and how do they differ?\",\n",
    "    \"How are low-quality QA pairs filtered in the pipeline?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    ask_rag_question(question, top_k=3, temperature=0.1)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec5cc2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Question: What datasets are mentioned in the SatcomLLM pipeline?\n",
      "================================================================================\n",
      "\n",
      "🔍 Searching knowledge base...\n",
      "\n",
      "📚 Retrieved 3 relevant documents:\n",
      "  [1] Synthetic QA Generation for ESA Satcom LLM > Official Datasets (score: 0.833)\n",
      "  [2] Synthetic QA Generation for ESA Satcom LLM > Official Datasets (score: 0.833)\n",
      "  [3] Synthetic QA Generation for ESA Satcom LLM > Pipeline Overview (score: 0.792)\n",
      "\n",
      "🤖 Generating answer with SatcomLLM...\n",
      "Job submitted: 6efc5964-e184-416e-91ab-80aa898af176-e2\n",
      "Waiting for response...\n",
      "\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "ANSWER:\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "The SatcomLLM pipeline mentions the following datasets:\n",
      "\n",
      "1. **esa-satcom-synth-qa** on HuggingFace (a synthetic QA dataset).\n",
      "\n",
      "There is no mention of other specific datasets besides this one. The term \"official datasets\" is mentioned, implying that esa-satcom-synth-qa is the primary dataset referred to, but no others are explicitly mentioned in the provided context.\n",
      "\n",
      "hosts, formats, and versions of this dataset are not mentioned in the provided documents.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The SatcomLLM pipeline mentions the following datasets:\\n\\n1. **esa-satcom-synth-qa** on HuggingFace (a synthetic QA dataset).\\n\\nThere is no mention of other specific datasets besides this one. The term \"official datasets\" is mentioned, implying that esa-satcom-synth-qa is the primary dataset referred to, but no others are explicitly mentioned in the provided context.\\n\\nhosts, formats, and versions of this dataset are not mentioned in the provided documents.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask your own question!\n",
    "my_question = \"What datasets are mentioned in the SatcomLLM pipeline?\"\n",
    "\n",
    "# Adjust parameters as needed:\n",
    "# - top_k: Number of documents to retrieve (3-5 recommended)\n",
    "# - temperature: 0.0 = deterministic, 1.0 = creative\n",
    "# - max_tokens: Maximum length of answer\n",
    "ask_rag_question(\n",
    "    question=my_question,\n",
    "    top_k=3,\n",
    "    temperature=0.1,\n",
    "    max_tokens=512,\n",
    "    show_sources=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021a56",
   "metadata": {},
   "source": [
    "## Summary: Complete RAG Pipeline\n",
    "\n",
    "Congratulations! You've built a complete RAG (Retrieval-Augmented Generation) system with:\n",
    "\n",
    "### Components:\n",
    "1. **Document Processing**: Markdown chunking with header-based splitting\n",
    "2. **Embeddings**: DeepInfra API with BAAI/bge-large-en-v1.5 (1024 dimensions)\n",
    "3. **Vector Database**: Qdrant Cloud for scalable similarity search\n",
    "4. **LLM Generation**: RunPod vLLM endpoint with SatcomLLM model\n",
    "5. **RAG Orchestration**: Custom pipeline combining retrieval and generation\n",
    "\n",
    "### Workflow:\n",
    "```\n",
    "User Question\n",
    "    ↓\n",
    "1. Embed query (DeepInfra)\n",
    "    ↓\n",
    "2. Search vectors (Qdrant)\n",
    "    ↓\n",
    "3. Retrieve top-k documents\n",
    "    ↓\n",
    "4. Build enriched prompt\n",
    "    ↓\n",
    "5. Generate answer (vLLM)\n",
    "    ↓\n",
    "Answer with sources\n",
    "```\n",
    "\n",
    "\n",
    "### Next Steps:\n",
    "- Try different questions\n",
    "- Adjust top_k for more/less context\n",
    "- Experiment with temperature settings\n",
    "- Add more documents to the knowledge base\n",
    "- Implement re-ranking for better results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea1670",
   "metadata": {},
   "source": [
    "## Bonus: PDF to Markdown Conversion\n",
    "\n",
    "If you have PDF documents, here's a utility function to convert them to markdown for processing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "\n",
    "def pdf_to_markdown(pdf_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Convert PDF to markdown format.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        output_path: Optional path for markdown output\n",
    "    \n",
    "    Returns:\n",
    "        Markdown text content\n",
    "    \"\"\"\n",
    "    # Open PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    markdown_content = []\n",
    "    \n",
    "    # Extract text from each page\n",
    "    for page_num, page in enumerate(doc, 1):\n",
    "        # Extract text\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Add page marker\n",
    "        markdown_content.append(f\"\\n## Page {page_num}\\n\")\n",
    "        markdown_content.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    # Combine all content\n",
    "    full_markdown = \"\\n\".join(markdown_content)\n",
    "    \n",
    "    # Save to file if output path provided\n",
    "    if output_path:\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_markdown)\n",
    "        print(f\"Markdown saved to: {output_path}\")\n",
    "    \n",
    "    return full_markdown\n",
    "\n",
    "# Example usage:\n",
    "# pdf_path = \"./data/sample_document.pdf\"\n",
    "# markdown_text = pdf_to_markdown(pdf_path, \"./data/sample_document.md\")\n",
    "# # Then use chunk_markdown_document(markdown_text) to chunk it\n",
    "\n",
    "print(\"PDF to Markdown converter ready!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
