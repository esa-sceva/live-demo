{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SatcomLLM: Fully Local RAG Pipeline Demo\n",
        "\n",
        "Welcome to the **Fully Local** SatcomLLM RAG Demo! This notebook demonstrates how to build a complete Retrieval-Augmented Generation (RAG) system that runs **entirely on your local machine** - no external APIs required.\n",
        "\n",
        "## What's Different from the Cloud Version?\n",
        "\n",
        "This local version replaces all cloud services with local alternatives:\n",
        "\n",
        "| Component | Cloud Version | Local Version |\n",
        "|-----------|---------------|---------------|\n",
        "| **Embeddings** | DeepInfra API | sentence-transformers (local) |\n",
        "| **Vector DB** | Qdrant Cloud | Qdrant Local (in-memory or file-based) |\n",
        "| **LLM Inference** | RunPod vLLM API | Local transformers (TinyLlama or similar) |\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "This notebook demonstrates how to build a RAG system that:\n",
        "- Runs completely offline\n",
        "- Uses local embedding models for semantic search\n",
        "- Stores vectors in a local Qdrant instance\n",
        "- Generates answers using a small local LLM (TinyLlama)\n",
        "- Works on Apple Silicon with MPS acceleration\n",
        "- **Saves and reuses processed chunks** to avoid re-processing\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "The demo is divided into 4 main parts:\n",
        "\n",
        "- **Setup and LLM Fundamentals**: Install dependencies, configure local models, and understand core concepts like tokenization and prompting. You'll test local model inference to ensure everything works.\n",
        "\n",
        "- **Theory around RAG**: Understand the principles of Retrieval-Augmented Generation (RAG), including how retrieval complements generative models.\n",
        "\n",
        "- **Embedding Vector Database Creation**: Load and chunk markdown documents, generate embeddings using local models, create a local Qdrant vector database, and populate it with embedded document chunks. **Chunks are saved locally for reuse.**\n",
        "\n",
        "- **Retrieval and RAG Pipeline**: Implement semantic search to find relevant documents, build the complete RAG pipeline that combines retrieval with local generation, and test the system with real questions.\n",
        "\n",
        "## Technology Stack\n",
        "\n",
        "- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2 or similar)\n",
        "- **Vector Database**: Qdrant Local (in-memory or persistent)\n",
        "- **LLM**: TinyLlama-1.1B-Chat-v1.0 (or any small local model)\n",
        "- **Document Processing**: LangChain's header-based chunking\n",
        "- **Chunk Storage**: Local JSON files for persistence\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.8 or higher\n",
        "- Apple Silicon Mac (for MPS) or CPU/GPU setup\n",
        "- ~2-4GB RAM for models\n",
        "- The complete demo takes approximately 5-10 minutes to run\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PART 1: Setup & LLM Fundamentals\n",
        "\n",
        "Before building our local RAG system, let's set up the environment and understand key LLM concepts.\n",
        "\n",
        "## What's in This Section?\n",
        "\n",
        "### 1. Dependency Installation\n",
        "Install all required Python packages:\n",
        "- `transformers` & `torch`: LLM libraries\n",
        "- `sentence-transformers`: Local embedding models\n",
        "- `langchain`: RAG orchestration  \n",
        "- `qdrant-client`: Local vector database\n",
        "- `python-dotenv`: Environment management\n",
        "\n",
        "### 2. Local Model Configuration\n",
        "Set up local models:\n",
        "- **Embedding Model**: sentence-transformers (runs locally)\n",
        "- **LLM Model**: TinyLlama or similar small model (runs locally)\n",
        "- **Vector Database**: Qdrant local instance (in-memory or file-based)\n",
        "\n",
        "### 3. LLM Concepts\n",
        "Learn the fundamentals:\n",
        "- **Tokenization**: Text → Numbers\n",
        "- **Prompting**: Crafting instructions\n",
        "- **Generation**: Controlling output\n",
        "\n",
        "### 4. Local Inference Testing\n",
        "Verify your local model setup works before building RAG.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Setup Instructions\n",
        "\n",
        "The cell below will automatically:\n",
        "1. **Create a virtual environment** (`./venv/`) if it doesn't exist\n",
        "2. **Install all required packages** into the venv\n",
        "3. **Add the venv to Python path** so packages are available in this notebook\n",
        "\n",
        "**First run**: This will take 2-5 minutes to download and install packages.\n",
        "**Subsequent runs**: This will be much faster (packages are already installed).\n",
        "\n",
        "Run the cell below to set up the environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Virtual environment already exists at venv\n",
            "\n",
            "Upgrading pip...\n",
            "\n",
            "Installing required packages (this may take a few minutes on first run)...\n",
            "All packages installed successfully!\n",
            "\n",
            "Environment setup complete! You can now run the following cells.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Define venv path\n",
        "VENV_PATH = Path(\"./venv\")\n",
        "VENV_PYTHON = VENV_PATH / \"bin\" / \"python\"\n",
        "VENV_PIP = VENV_PATH / \"bin\" / \"pip\"\n",
        "\n",
        "def find_system_python():\n",
        "    \"\"\"Find a working system Python 3 to create the venv.\"\"\"\n",
        "    python_candidates = [\n",
        "        \"/usr/bin/python3\",\n",
        "        \"/usr/local/bin/python3\", \n",
        "        \"/opt/homebrew/bin/python3\",\n",
        "        shutil.which(\"python3\"),\n",
        "    ]\n",
        "    \n",
        "    for python_path in python_candidates:\n",
        "        if python_path and Path(python_path).exists():\n",
        "            try:\n",
        "                result = subprocess.run(\n",
        "                    [python_path, \"--version\"], \n",
        "                    capture_output=True, text=True, timeout=5\n",
        "                )\n",
        "                if result.returncode == 0 and \"Python 3\" in result.stdout:\n",
        "                    return python_path\n",
        "            except Exception:\n",
        "                continue\n",
        "    return sys.executable\n",
        "\n",
        "# Step 1: Check if venv exists and is valid\n",
        "venv_valid = False\n",
        "if VENV_PATH.exists():\n",
        "    if VENV_PYTHON.exists():\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [str(VENV_PYTHON), \"--version\"], \n",
        "                capture_output=True, text=True, timeout=5\n",
        "            )\n",
        "            venv_valid = result.returncode == 0\n",
        "        except Exception:\n",
        "            venv_valid = False\n",
        "    \n",
        "    if not venv_valid:\n",
        "        print(\"Existing venv is broken, removing it...\")\n",
        "        shutil.rmtree(VENV_PATH, ignore_errors=True)\n",
        "\n",
        "# Step 2: Create virtual environment if needed\n",
        "if not VENV_PATH.exists():\n",
        "    print(\"Creating virtual environment...\")\n",
        "    system_python = find_system_python()\n",
        "    print(f\"  Using Python: {system_python}\")\n",
        "    subprocess.run([system_python, \"-m\", \"venv\", str(VENV_PATH)], check=True)\n",
        "    print(f\"Virtual environment created at {VENV_PATH}\")\n",
        "else:\n",
        "    print(f\"Virtual environment already exists at {VENV_PATH}\")\n",
        "\n",
        "# Step 3: Upgrade pip in venv\n",
        "print(\"\\nUpgrading pip...\")\n",
        "subprocess.run([str(VENV_PIP), \"install\", \"--upgrade\", \"pip\", \"-q\"], check=True)\n",
        "\n",
        "# Step 4: Install required packages\n",
        "print(\"\\nInstalling required packages (this may take a few minutes on first run)...\")\n",
        "packages = [\n",
        "    \"torch\",\n",
        "    \"transformers\",\n",
        "    \"accelerate\",\n",
        "    \"sentence-transformers\",\n",
        "    \"qdrant-client\",\n",
        "    \"langchain\",\n",
        "    \"langchain-community\",\n",
        "    \"langchain-text-splitters\",\n",
        "    \"tqdm\",\n",
        "    \"python-dotenv\"\n",
        "]\n",
        "\n",
        "subprocess.run([str(VENV_PIP), \"install\", \"-q\"] + packages, check=True)\n",
        "print(\"All packages installed successfully!\")\n",
        "\n",
        "# Step 5: Add venv to Python path for this notebook session\n",
        "venv_site_packages = None\n",
        "for python_dir in (VENV_PATH / \"lib\").glob(\"python*\"):\n",
        "    sp = python_dir / \"site-packages\"\n",
        "    if sp.exists():\n",
        "        venv_site_packages = sp\n",
        "        break\n",
        "\n",
        "if venv_site_packages and str(venv_site_packages) not in sys.path:\n",
        "    sys.path.insert(0, str(venv_site_packages))\n",
        "    print(f\"Added {venv_site_packages} to Python path\")\n",
        "\n",
        "print(\"\\nEnvironment setup complete! You can now run the following cells.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/riccardocorrente/live-demo/local-demo/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/riccardocorrente/live-demo/local-demo/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All required packages are installed correctly\n",
            "PyTorch version: 2.8.0\n",
            "Transformers version: 4.57.3\n",
            "MPS (Apple Silicon GPU) is available and will be used\n"
          ]
        }
      ],
      "source": [
        "# Ensure venv is in path (in case this cell is run after kernel restart)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "VENV_PATH = Path(\"./venv\")\n",
        "for python_dir in (VENV_PATH / \"lib\").glob(\"python*\"):\n",
        "    sp = python_dir / \"site-packages\"\n",
        "    if sp.exists() and str(sp) not in sys.path:\n",
        "        sys.path.insert(0, str(sp))\n",
        "\n",
        "# Verify imports work correctly\n",
        "try:\n",
        "    import transformers\n",
        "    import torch\n",
        "    import sentence_transformers\n",
        "    import langchain\n",
        "    import qdrant_client\n",
        "    import json\n",
        "    print(\"All required packages are installed correctly\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"Transformers version: {transformers.__version__}\")\n",
        "    \n",
        "    # Check MPS availability\n",
        "    if torch.backends.mps.is_available():\n",
        "        print(\"MPS (Apple Silicon GPU) is available and will be used\")\n",
        "    else:\n",
        "        print(\"MPS not available, will use CPU\")\n",
        "except ImportError as e:\n",
        "    print(f\"Missing package: {e}\")\n",
        "    print(\"\\nPlease run the setup cell above first (Cell 3)\")\n",
        "    print(\"This will create a virtual environment and install all packages.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Configure Local Models\n",
        "\n",
        "For this local demo, we'll use:\n",
        "\n",
        "1. **Embedding Model**: `all-MiniLM-L6-v2` (80MB, fast, good quality)\n",
        "   - Alternative: `all-mpnet-base-v2` (420MB, better quality)\n",
        "   \n",
        "2. **LLM Model**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (2.2GB, fast inference)\n",
        "   - Alternative: Any small model that fits in your RAM\n",
        "\n",
        "3. **Vector Database**: Qdrant Local (in-memory or file-based)\n",
        "\n",
        "4. **Chunk Storage**: Local JSON files in `./chunks_cache/` folder\n",
        "\n",
        "No API keys needed! Everything runs locally.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Local Storage Locations\n",
        "\n",
        "Before we proceed, it's important to understand **where everything is stored** on your local machine:\n",
        "\n",
        "#### 1. **Model Storage (HuggingFace Cache)**\n",
        "\n",
        "When you load models using `transformers` or `sentence-transformers`, they are automatically downloaded and cached locally:\n",
        "\n",
        "- **Default Location**: `~/.cache/huggingface/` (on macOS/Linux) or `C:\\Users\\<username>\\.cache\\huggingface\\` (on Windows)\n",
        "- **What's Stored**:\n",
        "  - **LLM Models**: TinyLlama (~2.2GB) stored in `models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/`\n",
        "  - **Embedding Models**: all-MiniLM-L6-v2 (~80MB) stored in `sentence-transformers/`\n",
        "  - **Tokenizers**: Vocabulary files and tokenizer configs\n",
        "\n",
        "- **First Run**: Models are downloaded from HuggingFace Hub (requires internet)\n",
        "- **Subsequent Runs**: Models are loaded from cache (no download needed)\n",
        "- **Disk Space**: ~2.3GB total for both models\n",
        "\n",
        "You can check your cache location:\n",
        "```python\n",
        "from huggingface_hub.constants import HF_HUB_CACHE\n",
        "print(f\"Models cached at: {HF_HUB_CACHE}\")\n",
        "```\n",
        "\n",
        "#### 2. **Vector Database Storage (Qdrant)**\n",
        "\n",
        "The vector database can be stored in two ways:\n",
        "\n",
        "- **In-Memory** (Current Setup): \n",
        "  - Location: RAM only\n",
        "  - **Pros**: Fast, no disk I/O\n",
        "  - **Cons**: Data is lost when notebook restarts\n",
        "  - Use: `QdrantClient(\":memory:\")`\n",
        "\n",
        "- **Persistent Storage** (Recommended for Production):\n",
        "  - Location: `./qdrant_db/` (or any path you specify)\n",
        "  - **Pros**: Data persists between sessions\n",
        "  - **Cons**: Slightly slower, uses disk space\n",
        "  - Use: `QdrantClient(path=\"./qdrant_db\")`\n",
        "  - **Disk Space**: ~10-50MB per 1000 document chunks (depends on embedding dimension)\n",
        "\n",
        "#### 3. **Chunk Storage (NEW - Local JSON Files)**\n",
        "\n",
        "Processed document chunks are now saved locally to avoid re-processing:\n",
        "\n",
        "- **Location**: `./chunks_cache/` folder\n",
        "- **Format**: JSON files (one per document)\n",
        "- **What's Stored**: \n",
        "  - Chunk text content\n",
        "  - Metadata (headers, source file, chunk IDs)\n",
        "  - Chunk size information\n",
        "- **Benefits**: \n",
        "  - Skip re-chunking on subsequent runs\n",
        "  - Fast loading of pre-processed chunks\n",
        "  - Easy to inspect and debug\n",
        "- **Disk Space**: ~1-5MB per 1000 chunks (text only, no embeddings)\n",
        "\n",
        "#### 4. **Document Storage**\n",
        "\n",
        "- **Source Documents**: Your markdown files in `data/` folder\n",
        "- **Chunked Documents**: Stored in Qdrant payload (along with embeddings) AND saved as JSON files\n",
        "- **Metadata**: Document headers, source files, chunk IDs stored in both places\n",
        "\n",
        "#### Summary of Storage Locations:\n",
        "\n",
        "```\n",
        "Your Project/\n",
        "├── data/                          # Your source markdown files\n",
        "│   └── *.md\n",
        "├── chunks_cache/                  # Saved processed chunks (NEW!)\n",
        "│   └── <document_name>_chunks.json\n",
        "├── qdrant_db/                     # Vector database (if using persistent storage)\n",
        "│   └── collections/\n",
        "│       └── satcom_rag_local/\n",
        "└── ~/.cache/huggingface/          # Model cache (automatic)\n",
        "    ├── models--TinyLlama--.../    # LLM model (~2.2GB)\n",
        "    └── sentence-transformers/     # Embedding model (~80MB)\n",
        "```\n",
        "\n",
        "**Tip**: To free up space, you can delete the HuggingFace cache, but models will need to be re-downloaded on next use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MPS (Apple Silicon GPU)\n",
            "\n",
            "Configuration:\n",
            "  Embedding Model: all-MiniLM-L6-v2\n",
            "  LLM Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "  Device: mps\n",
            "  Chunks Cache: chunks_cache\n",
            "\n",
            "All models will run locally - no API calls needed!\n"
          ]
        }
      ],
      "source": [
        "# Ensure venv is in path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "VENV_PATH = Path(\"./venv\")\n",
        "for python_dir in (VENV_PATH / \"lib\").glob(\"python*\"):\n",
        "    sp = python_dir / \"site-packages\"\n",
        "    if sp.exists() and str(sp) not in sys.path:\n",
        "        sys.path.insert(0, str(sp))\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Configuration for local models\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  # Small, fast embedding model\n",
        "LLM_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small LLM\n",
        "\n",
        "# Storage paths\n",
        "CHUNKS_CACHE_DIR = Path(\"./chunks_cache\")\n",
        "CHUNKS_CACHE_DIR.mkdir(exist_ok=True)  # Create chunks cache directory\n",
        "\n",
        "# Device configuration\n",
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"\n",
        "    print(\"Using MPS (Apple Silicon GPU)\")\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "    print(\"Using CUDA (NVIDIA GPU)\")\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  Embedding Model: {EMBEDDING_MODEL_NAME}\")\n",
        "print(f\"  LLM Model: {LLM_MODEL_NAME}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Chunks Cache: {CHUNKS_CACHE_DIR}\")\n",
        "print(f\"\\nAll models will run locally - no API calls needed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Tokenization: Breaking Text into Tokens\n",
        "\n",
        "Before a language model can process text, the text must be converted into a form the model can understand. This is why **tokenization** is essential: it transforms raw text into numerical units that the model can operate on.\n",
        "\n",
        "Let's see tokenization in action with our local model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer for TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
            "\n",
            "Original text: Satellite communications enable global connectivity through geostationary and low Earth orbit satellites.\n",
            "\n",
            "Number of tokens: 22\n",
            "\n",
            "Tokens: ['<s>', '▁Sat', 'ellite', '▁communic', 'ations', '▁enable', '▁global', '▁connect', 'ivity', '▁through', '▁ge', 'ost', 'ation', 'ary', '▁and', '▁low', '▁Earth', '▁orbit', '▁sat', 'ell', 'ites', '.']\n",
            "\n",
            "Token IDs: [1, 12178, 20911, 7212, 800, 9025, 5534, 4511, 2068, 1549, 1737, 520, 362, 653, 322, 4482, 11563, 16980, 3290, 514, 3246, 29889]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer for TinyLlama (will be downloaded on first run)\n",
        "print(f\"Loading tokenizer for {LLM_MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "\n",
        "# Example text about satellite communications\n",
        "text = \"Satellite communications enable global connectivity through geostationary and low Earth orbit satellites.\"\n",
        "\n",
        "# Tokenize the text\n",
        "encoded = tokenizer(\n",
        "    text,\n",
        "    return_offsets_mapping=True,\n",
        "    return_tensors=\"pt\",\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "# Convert token IDs back to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0])\n",
        "\n",
        "print(f\"\\nOriginal text: {text}\\n\")\n",
        "print(f\"Number of tokens: {len(tokens)}\\n\")\n",
        "print(f\"Tokens: {tokens}\\n\")\n",
        "print(f\"Token IDs: {encoded['input_ids'][0].tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Local LLM Inference\n",
        "\n",
        "Now let's set up and test local LLM inference. We'll create a simple function to generate text using our local model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Loading Process Explained\n",
        "\n",
        "When you run the cell above, here's what happens:\n",
        "\n",
        "1. **Check Cache**: First, the code checks if the model exists in `~/.cache/huggingface/`\n",
        "2. **Download (if needed)**: If not cached, downloads from HuggingFace Hub\n",
        "   - TinyLlama: ~2.2GB download (one-time)\n",
        "   - Progress bar shows download status\n",
        "3. **Load to Memory**: Model weights loaded into RAM/GPU memory\n",
        "4. **Device Assignment**: Model moved to MPS (Apple Silicon), CUDA (NVIDIA), or CPU\n",
        "\n",
        "**Memory Usage**:\n",
        "- **Model Size on Disk**: ~2.2GB (quantized/compressed)\n",
        "- **Model Size in RAM**: ~2.2-4GB (depending on precision: float32 vs float16)\n",
        "- **With MPS**: Uses GPU memory instead of RAM (faster inference)\n",
        "\n",
        "**First Run**: Expect 2-5 minutes for download (depends on internet speed)\n",
        "**Subsequent Runs**: Expect 10-30 seconds for loading from cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading local LLM model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "This may take a moment on first run (downloading model)...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use mps\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local LLM model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Load local LLM model\n",
        "print(f\"Loading local LLM model: {LLM_MODEL_NAME}\")\n",
        "print(\"This may take a moment on first run (downloading model)...\\n\")\n",
        "\n",
        "local_llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if DEVICE != \"cpu\" else torch.float32,\n",
        "    device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Move to device if not using device_map\n",
        "if DEVICE != \"cuda\":\n",
        "    if DEVICE == \"mps\":\n",
        "        local_llm_model = local_llm_model.to(\"mps\")\n",
        "    else:\n",
        "        local_llm_model = local_llm_model.to(\"cpu\")\n",
        "\n",
        "# Create text generation pipeline\n",
        "if DEVICE == \"mps\":\n",
        "    pipeline_device = torch.device(\"mps\")\n",
        "elif DEVICE == \"cuda\":\n",
        "    pipeline_device = 0\n",
        "else:\n",
        "    pipeline_device = -1\n",
        "\n",
        "local_text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=local_llm_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=pipeline_device\n",
        ")\n",
        "\n",
        "print(\"Local LLM model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing local LLM inference...\n",
            "\n",
            "Question: What is satellite communications in one sentence?\n",
            "\n",
            "Response: Satellite communications refers to the use of space-based communication systems to send and receive data over long distances.\n",
            "\n",
            "Local LLM inference working correctly!\n"
          ]
        }
      ],
      "source": [
        "def chat_with_local_llm(prompt, max_new_tokens=256, temperature=0.2, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate text using local LLM.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Input text prompt\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        top_p: Top-p sampling parameter\n",
        "    \n",
        "    Returns:\n",
        "        Generated text\n",
        "    \"\"\"\n",
        "    # Format prompt for chat model\n",
        "    formatted_prompt = f\"<|user|>\\n{prompt}<|assistant|>\\n\"\n",
        "    \n",
        "    # Generate\n",
        "    outputs = local_text_generator(\n",
        "        formatted_prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Extract generated text\n",
        "    generated_text = outputs[0][\"generated_text\"]\n",
        "    \n",
        "    # Remove the prompt from the output\n",
        "    if formatted_prompt in generated_text:\n",
        "        answer = generated_text.split(formatted_prompt)[-1].strip()\n",
        "    else:\n",
        "        answer = generated_text.strip()\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# Test local LLM\n",
        "print(\"Testing local LLM inference...\\n\")\n",
        "test_prompt = \"What is satellite communications in one sentence?\"\n",
        "print(f\"Question: {test_prompt}\\n\")\n",
        "\n",
        "response = chat_with_local_llm(test_prompt, max_new_tokens=128, temperature=0.7)\n",
        "print(f\"Response: {response}\\n\")\n",
        "print(\"Local LLM inference working correctly!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PART 2: THEORY AROUND RAG SYSTEM\n",
        "\n",
        "Now that we know that the local model is working, let's understand how to implement RAG with **custom documents**.\n",
        "\n",
        "## 2.1. The Problem with Standard LLMs\n",
        "\n",
        "Large Language Models have limitations:\n",
        "\n",
        "| Issue | Description | Impact |\n",
        "|-------|-------------|--------|\n",
        "| **Hallucinations** | Generate plausible but false info | Unreliable answers |\n",
        "| **Knowledge Cutoff** | Training data has a date limit | Outdated information |\n",
        "| **No Source** | Can't cite where info came from | Unverifiable |\n",
        "| **Generic** | Lack domain-specific expertise | Poor specialized answers |\n",
        "| **Static** | Can't update without retraining | Expensive to maintain |\n",
        "\n",
        "## How RAG Solves These Problems\n",
        "\n",
        "**Retrieval-Augmented Generation** adds a knowledge retrieval step:\n",
        "\n",
        "```\n",
        "Standard LLM:\n",
        "Question → LLM → Answer (may hallucinate)\n",
        "\n",
        "RAG System:\n",
        "Question → Find Relevant Docs → LLM + Context → Grounded Answer\n",
        "```\n",
        "\n",
        "### Key Benefits:\n",
        "\n",
        "- Grounded: Answers based on actual documents  \n",
        "- Verifiable: Shows sources used  \n",
        "- Up-to-date: Update docs without retraining model  \n",
        "- Domain-specific: Add specialized knowledge  \n",
        "- Cost-effective: Cheaper than fine-tuning  \n",
        "\n",
        "## 2.2 RAG Architecture\n",
        "\n",
        "### Two Main Phases:\n",
        "\n",
        "#### Phase 1: Indexing (One-Time Setup)\n",
        "```\n",
        "Documents → Clean → Chunk → Embed → Store in Vector DB\n",
        "```\n",
        "\n",
        "#### Phase 2: Query (Every Question)\n",
        "```\n",
        "Question → Embed → Search Vector DB → Retrieve Docs → \n",
        "    Augment Prompt → LLM → Answer\n",
        "```\n",
        "\n",
        "## 2.3 Architecture Overview\n",
        "\n",
        "```\n",
        "┌─────────────┐\n",
        "│   User      │ Asks a question\n",
        "│  Question   │\n",
        "└──────┬──────┘\n",
        "       ↓\n",
        "┌──────────────────────────────────────────┐\n",
        "│  1. EMBED QUERY (Local sentence-transformers) │\n",
        "│     Convert question → vector           │\n",
        "└──────┬───────────────────────────────────┘\n",
        "       ↓\n",
        "┌──────────────────────────────────────────┐\n",
        "│  2. SEMANTIC SEARCH (Local Qdrant)       │\n",
        "│     Find top-k similar document chunks   │\n",
        "└──────┬───────────────────────────────────┘\n",
        "       ↓\n",
        "┌──────────────────────────────────────────┐\n",
        "│  3. RETRIEVE CONTEXT                     │\n",
        "│     Get text + metadata from matches     │\n",
        "└──────┬───────────────────────────────────┘\n",
        "       ↓\n",
        "┌──────────────────────────────────────────┐\n",
        "│  4. ENRICH PROMPT                        │\n",
        "│     Question + Retrieved Context         │\n",
        "└──────┬───────────────────────────────────┘\n",
        "       ↓\n",
        "┌──────────────────────────────────────────┐\n",
        "│  5. GENERATE ANSWER (Local LLM)          │\n",
        "│     TinyLlama produces grounded response │\n",
        "└──────┬───────────────────────────────────┘\n",
        "       ↓\n",
        "┌──────────────┐\n",
        "│   Answer     │ With source attribution\n",
        "│ + Sources    │\n",
        "└──────────────┘\n",
        "```\n",
        "\n",
        "Let's start building!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Embedding & Vector Database\n",
        "\n",
        "In this part, we'll build the knowledge base using **local** components:\n",
        "\n",
        "1. **Load documents** from local files (Markdown).\n",
        "2. **Chunk the documents** intelligently to preserve semantic structure.\n",
        "3. **Save chunks locally** (NEW!) to avoid re-processing.\n",
        "4. **Generate embeddings** for each chunk using local sentence-transformers.\n",
        "5. **Upload the embeddings to a local Qdrant database**, making them ready for retrieval.\n",
        "\n",
        "## 3.1. Document Loading and Chunking\n",
        "\n",
        "Let's load and chunk our documents. **Chunks will be saved locally for reuse!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4 markdown file(s) in 'data':\n",
            "  - dace33f5-f959-4955-bd68-00229c97599e.md\n",
            "  - ecf0399d-1463-405b-99a3-6878f4828bd7.md\n",
            "  - db5563aa-30f9-4b6d-a9de-f8a22f5d30f4.md\n",
            "  - ea51d98e-f644-4fbb-8558-a661ce56cd9e.md\n",
            "\n",
            "dace33f5-f959-4955-bd68-00229c97599e.md: 24472 characters\n",
            "\n",
            "ecf0399d-1463-405b-99a3-6878f4828bd7.md: 59954 characters\n",
            "\n",
            "db5563aa-30f9-4b6d-a9de-f8a22f5d30f4.md: 16223 characters\n",
            "\n",
            "ea51d98e-f644-4fbb-8558-a661ce56cd9e.md: 21191 characters\n",
            "\n",
            "Total characters across all documents: 121840\n",
            "\n",
            "Preview of first document:\n",
            "# Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning\n",
            "\n",
            "[PERSON]\n",
            "\n",
            "_Monta Vista High School_\n",
            "\n",
            "Cupertino, CA, United States\n",
            "\n",
            "###### Abstract\n",
            "\n",
            "Since frequent severe droughts are lengthening the dry season in the Amazon Rainforest, it is important to detect wildfires promptly and forecast possible spread for effective suppression response. Current wildfire detection models are not versatile enough for the low-technology conditions of South American hot spots. Thi\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Load all markdown documents from the data folder\n",
        "data_folder = Path('data')\n",
        "\n",
        "# Check if data folder exists\n",
        "if not data_folder.exists():\n",
        "    print(f\"Data folder not found: {data_folder}\")\n",
        "    print(\"Creating sample data folder...\")\n",
        "    data_folder.mkdir(exist_ok=True)\n",
        "    print(\"Please add your markdown files to the 'data' folder\")\n",
        "    documents = {}\n",
        "else:\n",
        "    # Find all markdown files\n",
        "    markdown_files = list(data_folder.glob('*.md')) + list(data_folder.glob('*.markdown'))\n",
        "    \n",
        "    if not markdown_files:\n",
        "        print(f\"No markdown files found in {data_folder}\")\n",
        "        print(\"Please add markdown files to the 'data' folder\")\n",
        "        documents = {}\n",
        "    else:\n",
        "        print(f\"Found {len(markdown_files)} markdown file(s) in '{data_folder}':\")\n",
        "        for f in markdown_files:\n",
        "            print(f\"  - {f.name}\")\n",
        "        \n",
        "        # Load all documents\n",
        "        documents = {}\n",
        "        total_chars = 0\n",
        "        \n",
        "        for file_path in markdown_files:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                documents[file_path.name] = content\n",
        "                total_chars += len(content)\n",
        "                print(f\"\\n{file_path.name}: {len(content)} characters\")\n",
        "        \n",
        "        print(f\"\\nTotal characters across all documents: {total_chars}\")\n",
        "        if documents:\n",
        "            print(f\"\\nPreview of first document:\")\n",
        "            first_doc = list(documents.values())[0]\n",
        "            print(first_doc[:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing dace33f5-f959-4955-bd68-00229c97599e.md...\n",
            "  Generated 40 chunks\n",
            "  ✓ Saved to chunks_cache/dace33f5-f959-4955-bd68-00229c97599e_chunks.json\n",
            "\n",
            "Processing ecf0399d-1463-405b-99a3-6878f4828bd7.md...\n",
            "  Generated 86 chunks\n",
            "  ✓ Saved to chunks_cache/ecf0399d-1463-405b-99a3-6878f4828bd7_chunks.json\n",
            "\n",
            "Processing db5563aa-30f9-4b6d-a9de-f8a22f5d30f4.md...\n",
            "  Generated 22 chunks\n",
            "  ✓ Saved to chunks_cache/db5563aa-30f9-4b6d-a9de-f8a22f5d30f4_chunks.json\n",
            "\n",
            "Processing ea51d98e-f644-4fbb-8558-a661ce56cd9e.md...\n",
            "  Generated 31 chunks\n",
            "  ✓ Saved to chunks_cache/ea51d98e-f644-4fbb-8558-a661ce56cd9e_chunks.json\n",
            "\n",
            "============================================================\n",
            "Total chunks across all documents: 179\n",
            "============================================================\n",
            "\n",
            "Sample chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: dace33f5-f959-4955-bd68-00229c97599e.md\n",
            "Headers: Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning > \n",
            "Content preview: [PERSON]  \n",
            "_Monta Vista High School_  \n",
            "Cupertino, CA, United States  \n",
            "###### Abstract...\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: dace33f5-f959-4955-bd68-00229c97599e.md\n",
            "Headers: Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning > \n",
            "Content preview: Since frequent severe droughts are lengthening the dry season in the Amazon Rainforest, it is important to detect wildfires promptly and forecast poss...\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Source: dace33f5-f959-4955-bd68-00229c97599e.md\n",
            "Headers: Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning > \n",
            "Content preview: differing degrees of cirrus cloud contamination. Three additional Convolutional Neural Networks are trained to conduct a sensitivity analysis measurin...\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def chunk_markdown_document(markdown_text, max_chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Chunk markdown document using header-based splitting.\n",
        "    \n",
        "    Args:\n",
        "        markdown_text: Markdown formatted text\n",
        "        max_chunk_size: Maximum chunk size in characters\n",
        "        chunk_overlap: Overlap between chunks for context preservation\n",
        "    \n",
        "    Returns:\n",
        "        List of Document objects with content and metadata\n",
        "    \"\"\"\n",
        "    # Define headers to split on\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "        (\"####\", \"Header 4\"),\n",
        "    ]\n",
        "    \n",
        "    # Initialize markdown splitter\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=headers_to_split_on\n",
        "    )\n",
        "    \n",
        "    # Split by headers\n",
        "    md_header_splits = markdown_splitter.split_text(markdown_text)\n",
        "    \n",
        "    # Further split large chunks if needed\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=max_chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "    )\n",
        "    \n",
        "    # Process each header-based chunk\n",
        "    all_chunks = []\n",
        "    for idx, doc in enumerate(md_header_splits):\n",
        "        # If chunk is too large, split further\n",
        "        if len(doc.page_content) > max_chunk_size:\n",
        "            sub_chunks = text_splitter.split_text(doc.page_content)\n",
        "            for sub_idx, sub_chunk in enumerate(sub_chunks):\n",
        "                chunk_doc = Document(\n",
        "                    page_content=sub_chunk,\n",
        "                    metadata={\n",
        "                        **doc.metadata,\n",
        "                        'chunk_id': f\"{idx}_{sub_idx}\",\n",
        "                        'chunk_size': len(sub_chunk)\n",
        "                    }\n",
        "                )\n",
        "                all_chunks.append(chunk_doc)\n",
        "        else:\n",
        "            doc.metadata['chunk_id'] = str(idx)\n",
        "            doc.metadata['chunk_size'] = len(doc.page_content)\n",
        "            all_chunks.append(doc)\n",
        "    \n",
        "    return all_chunks\n",
        "\n",
        "def save_chunks_to_file(chunks, filename):\n",
        "    \"\"\"\n",
        "    Save chunks to a JSON file for later reuse.\n",
        "    \n",
        "    Args:\n",
        "        chunks: List of Document objects\n",
        "        filename: Name of the file to save to\n",
        "    \"\"\"\n",
        "    chunks_data = []\n",
        "    for chunk in chunks:\n",
        "        chunks_data.append({\n",
        "            'page_content': chunk.page_content,\n",
        "            'metadata': chunk.metadata\n",
        "        })\n",
        "    \n",
        "    filepath = CHUNKS_CACHE_DIR / filename\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    return filepath\n",
        "\n",
        "def load_chunks_from_file(filename):\n",
        "    \"\"\"\n",
        "    Load chunks from a saved JSON file.\n",
        "    \n",
        "    Args:\n",
        "        filename: Name of the file to load from\n",
        "    \n",
        "    Returns:\n",
        "        List of Document objects\n",
        "    \"\"\"\n",
        "    filepath = CHUNKS_CACHE_DIR / filename\n",
        "    if not filepath.exists():\n",
        "        return None\n",
        "    \n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        chunks_data = json.load(f)\n",
        "    \n",
        "    chunks = []\n",
        "    for chunk_data in chunks_data:\n",
        "        chunk = Document(\n",
        "            page_content=chunk_data['page_content'],\n",
        "            metadata=chunk_data['metadata']\n",
        "        )\n",
        "        chunks.append(chunk)\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Chunk all documents (with caching!)\n",
        "if 'documents' in locals() and documents:\n",
        "    all_chunks = []\n",
        "    \n",
        "    for filename, markdown_text in documents.items():\n",
        "        # Check if chunks are already cached\n",
        "        cache_filename = f\"{Path(filename).stem}_chunks.json\"\n",
        "        cached_chunks = load_chunks_from_file(cache_filename)\n",
        "        \n",
        "        if cached_chunks:\n",
        "            print(f\"\\n✓ Loading cached chunks for {filename} ({len(cached_chunks)} chunks)\")\n",
        "            all_chunks.extend(cached_chunks)\n",
        "        else:\n",
        "            print(f\"\\nProcessing {filename}...\")\n",
        "            doc_chunks = chunk_markdown_document(markdown_text)\n",
        "            \n",
        "            # Add source filename to metadata\n",
        "            for chunk in doc_chunks:\n",
        "                chunk.metadata['source_file'] = filename\n",
        "            \n",
        "            # Save chunks to cache\n",
        "            saved_path = save_chunks_to_file(doc_chunks, cache_filename)\n",
        "            print(f\"  Generated {len(doc_chunks)} chunks\")\n",
        "            print(f\"  ✓ Saved to {saved_path}\")\n",
        "            \n",
        "            all_chunks.extend(doc_chunks)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Total chunks across all documents: {len(all_chunks)}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Show sample chunks\n",
        "    if all_chunks:\n",
        "        print(\"\\nSample chunks:\")\n",
        "        for i, chunk in enumerate(all_chunks[:3]):\n",
        "            print(f\"\\n--- Chunk {i+1} ---\")\n",
        "            print(f\"Source: {chunk.metadata.get('source_file', 'unknown')}\")\n",
        "            print(f\"Headers: {chunk.metadata.get('Header 1', '')} > {chunk.metadata.get('Header 2', '')}\")\n",
        "            print(f\"Content preview: {chunk.page_content[:150]}...\")\n",
        "    \n",
        "    # Rename for consistency\n",
        "    chunks = all_chunks\n",
        "else:\n",
        "    print(\"⚠ No documents loaded. Please add markdown files to the 'data' folder.\")\n",
        "    chunks = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunk Caching Explained\n",
        "\n",
        "The chunking process now includes **local caching**:\n",
        "\n",
        "#### How It Works:\n",
        "\n",
        "1. **Check Cache**: Before processing, checks if chunks exist in `./chunks_cache/<filename>_chunks.json`\n",
        "2. **Load from Cache**: If found, loads instantly (no re-processing!)\n",
        "3. **Process & Save**: If not found, processes document and saves chunks to cache\n",
        "4. **Reuse**: Next time you run the notebook, chunks load from cache\n",
        "\n",
        "#### Benefits:\n",
        "\n",
        "- **Fast**: Skip re-chunking on subsequent runs\n",
        "- **Consistent**: Same chunks every time\n",
        "- **Inspectable**: JSON files are human-readable\n",
        "- **Portable**: Can share cache files between projects\n",
        "\n",
        "#### Cache File Structure:\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"page_content\": \"Chunk text content...\",\n",
        "    \"metadata\": {\n",
        "      \"Header 1\": \"Section Title\",\n",
        "      \"Header 2\": \"Subsection\",\n",
        "      \"chunk_id\": \"0\",\n",
        "      \"chunk_size\": 856,\n",
        "      \"source_file\": \"document.md\"\n",
        "    }\n",
        "  },\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "#### Managing Cache:\n",
        "\n",
        "- **Clear cache**: Delete files from `./chunks_cache/` folder\n",
        "- **Force re-process**: Delete specific cache file or entire folder\n",
        "- **Share cache**: Copy `./chunks_cache/` folder to another project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Initialize Local Embeddings\n",
        "\n",
        "Now we'll set up our **local embedding model** using sentence-transformers. This runs entirely on your machine - no API calls!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding Model Storage\n",
        "\n",
        "The embedding model (`all-MiniLM-L6-v2`) is stored similarly:\n",
        "\n",
        "- **Cache Location**: `~/.cache/huggingface/sentence-transformers/`\n",
        "- **Model Size**: ~80MB\n",
        "- **First Download**: ~30 seconds\n",
        "- **Subsequent Loads**: ~2-5 seconds\n",
        "\n",
        "**Note on MPS**: sentence-transformers may not fully support MPS yet, so embeddings typically run on CPU. This is usually fast enough since embedding is a one-time operation per document chunk.\n",
        "\n",
        "**Embedding Dimensions**: \n",
        "- `all-MiniLM-L6-v2`: 384 dimensions\n",
        "- Each document chunk → 384-dimensional vector\n",
        "- Stored in Qdrant for similarity search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading local embedding model: all-MiniLM-L6-v2\n",
            "This may take a moment on first run (downloading model)...\n",
            "\n",
            "Note: Embeddings will use CPU (MPS support in sentence-transformers is limited)\n",
            "✓ Local embedding model loaded successfully!\n",
            "✓ Model: all-MiniLM-L6-v2\n",
            "✓ Embedding dimension: 384\n",
            "✓ Sample embedding (first 10 values): [-0.10052735  0.00207785 -0.10086767  0.00358008 -0.09882836  0.01954412\n",
            " -0.10562673 -0.0159261   0.02350388  0.02032893]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize local embedding model\n",
        "print(f\"Loading local embedding model: {EMBEDDING_MODEL_NAME}\")\n",
        "print(\"This may take a moment on first run (downloading model)...\\n\")\n",
        "\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# Move to device if available\n",
        "if DEVICE == \"mps\":\n",
        "    # Note: sentence-transformers may not fully support MPS yet, but will use CPU efficiently\n",
        "    print(\"Note: Embeddings will use CPU (MPS support in sentence-transformers is limited)\")\n",
        "elif DEVICE == \"cuda\":\n",
        "    embedding_model = embedding_model.to(DEVICE)\n",
        "\n",
        "# Test embedding\n",
        "sample_text = \"The SatcomLLM pipeline generates synthetic QA pairs from documents\"\n",
        "sample_embedding = embedding_model.encode(sample_text)\n",
        "\n",
        "print(f\"✓ Local embedding model loaded successfully!\")\n",
        "print(f\"✓ Model: {EMBEDDING_MODEL_NAME}\")\n",
        "print(f\"✓ Embedding dimension: {len(sample_embedding)}\")\n",
        "print(f\"✓ Sample embedding (first 10 values): {sample_embedding[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Create Local Qdrant Vector Database\n",
        "\n",
        "Time to set up our **local vector database** where we'll store all document embeddings.\n",
        "\n",
        "### Collection Configuration\n",
        "\n",
        "| Parameter | Value | Explanation |\n",
        "|-----------|-------|-------------|\n",
        "| **Name** | `satcom_rag_local` | Identifier for our knowledge base |\n",
        "| **Vector Size** | 384 | Matches all-MiniLM-L6-v2 output |\n",
        "| **Distance Metric** | Cosine | Best for normalized embeddings |\n",
        "| **Storage** | Local (in-memory or file-based) | Runs entirely on your machine |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database Storage Options\n",
        "\n",
        "Qdrant can store data in two ways. Currently, we're using **in-memory** storage, but you can easily switch to **persistent** storage:\n",
        "\n",
        "#### Option 1: In-Memory (Current - Fast but Temporary)\n",
        "```python\n",
        "qdrant_client = QdrantClient(\":memory:\")\n",
        "```\n",
        "- **Storage**: RAM only\n",
        "- **Persistence**: Lost when notebook restarts\n",
        "- **Speed**: Fastest (no disk I/O)\n",
        "- **Use Case**: Quick experiments, testing\n",
        "\n",
        "#### Option 2: Persistent File-Based (Recommended for Production)\n",
        "```python\n",
        "qdrant_client = QdrantClient(path=\"./qdrant_db\")\n",
        "```\n",
        "- **Storage**: Disk (`./qdrant_db/` folder)\n",
        "- **Persistence**: Survives restarts\n",
        "- **Speed**: Fast (local disk)\n",
        "- **Use Case**: Production, reusing data between sessions\n",
        "\n",
        "**To Switch to Persistent Storage**:\n",
        "1. Uncomment the persistent storage line in the code cell below\n",
        "2. Comment out the `:memory:` line\n",
        "3. Your vector database will be saved to `./qdrant_db/` folder\n",
        "4. Next time you run the notebook, it will load existing data\n",
        "\n",
        "**Disk Space Estimate**:\n",
        "- ~1000 chunks with 384-dim embeddings ≈ 10-20MB\n",
        "- ~10,000 chunks ≈ 100-200MB\n",
        "- Scales linearly with number of chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing local Qdrant vector database...\n",
            "Creating collection: satcom_rag_local\n",
            "Vector dimension: 384\n",
            "Collection 'satcom_rag_local' created successfully!\n"
          ]
        }
      ],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "import uuid\n",
        "\n",
        "# Initialize local Qdrant client (in-memory)\n",
        "# For persistent storage, use: QdrantClient(path=\"./qdrant_db\")\n",
        "print(\"Initializing local Qdrant vector database...\")\n",
        "# qdrant_client = QdrantClient(\":memory:\")  # In-memory storage (fast, but not persistent)\n",
        "# For persistent storage, uncomment:\n",
        "qdrant_client = QdrantClient(path=\"./qdrant_db\")\n",
        "\n",
        "# Collection name\n",
        "collection_name = \"satcom_rag_local\"\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dim = len(embedding_model.encode(\"test\"))\n",
        "\n",
        "# Create new collection\n",
        "print(f\"Creating collection: {collection_name}\")\n",
        "print(f\"Vector dimension: {embedding_dim}\")\n",
        "\n",
        "try:\n",
        "    qdrant_client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE),\n",
        "    )\n",
        "    print(f\"Collection '{collection_name}' created successfully!\")\n",
        "except Exception as e:\n",
        "    # Collection might already exist\n",
        "    if \"already exists\" in str(e).lower():\n",
        "        print(f\"Collection '{collection_name}' already exists, using existing collection\")\n",
        "    else:\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Embedding & Upload Process\n",
        "\n",
        "When you run the cell below, here's the step-by-step process:\n",
        "\n",
        "#### Step 1: Load Chunks (from cache or fresh)\n",
        "- Input: Chunks from cache files OR freshly processed\n",
        "- Process: Load from `./chunks_cache/` if available\n",
        "- Output: List of Document objects\n",
        "\n",
        "#### Step 2: Batch Embedding Generation\n",
        "- Input: Text chunks (batch of 32 at a time)\n",
        "- Process: Local embedding model converts text → vectors\n",
        "- Output: NumPy arrays of 384-dimensional vectors\n",
        "- **Storage**: Temporarily in RAM, then written to Qdrant\n",
        "\n",
        "#### Step 3: Vector Storage\n",
        "- Input: Embeddings + document text + metadata\n",
        "- Process: Create Qdrant PointStruct objects\n",
        "- Output: Points uploaded to Qdrant collection\n",
        "- **Storage**: \n",
        "  - In-memory: RAM\n",
        "  - Persistent: `./qdrant_db/` folder\n",
        "\n",
        "#### Data Flow:\n",
        "```\n",
        "Cached Chunks → Embeddings (384-dim vectors) → Qdrant Points → Vector DB\n",
        "```\n",
        "\n",
        "**Performance**:\n",
        "- Loading cached chunks: ~0.1-1 seconds\n",
        "- Embedding 100 chunks: ~5-10 seconds\n",
        "- Embedding 1000 chunks: ~1-2 minutes\n",
        "- Upload to Qdrant: ~1-5 seconds (depends on storage type)\n",
        "\n",
        "**Memory Usage During Processing**:\n",
        "- Embeddings temporarily in RAM: ~150KB per chunk\n",
        "- Qdrant points in memory: ~10-20KB per chunk\n",
        "- Total for 1000 chunks: ~150-200MB RAM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding and uploading 179 chunks to local Qdrant...\n",
            "This may take a moment...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chunks: 100%|██████████| 6/6 [00:00<00:00,  6.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uploading 179 points to local Qdrant...\n",
            "✓ Successfully uploaded 179 chunks to local Qdrant!\n",
            "\n",
            "Collection info:\n",
            "  - Points count: 179\n",
            "  - Vector size: 384\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Upload chunks to local Qdrant with embeddings\n",
        "if chunks:\n",
        "    # Check if collection already has data (for persistent storage)\n",
        "    collection_info = qdrant_client.get_collection(collection_name=collection_name)\n",
        "    existing_points = collection_info.points_count\n",
        "    \n",
        "    if existing_points > 0:\n",
        "        print(f\"ℹ Collection '{collection_name}' already contains {existing_points} points.\")\n",
        "        print(\"  Skipping upload - using existing data.\")\n",
        "        print(\"  (To re-upload: delete ./qdrant_db/ folder and restart)\")\n",
        "        SKIP_UPLOAD = True\n",
        "    else:\n",
        "        SKIP_UPLOAD = False\n",
        "    \n",
        "    if not SKIP_UPLOAD:\n",
        "        print(f\"Embedding and uploading {len(chunks)} chunks to local Qdrant...\")\n",
        "        print(\"This may take a moment...\\n\")\n",
        "        \n",
        "        # Prepare points for batch upload\n",
        "        points = []\n",
        "        \n",
        "        # Process chunks in batches\n",
        "        batch_size = 32  # Larger batches for local processing\n",
        "        for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing chunks\"):\n",
        "            batch_chunks = chunks[i:i+batch_size]\n",
        "            \n",
        "            # Get texts from batch\n",
        "            texts = [chunk.page_content for chunk in batch_chunks]\n",
        "            \n",
        "            # Generate embeddings for batch (local, no API calls!)\n",
        "            embeddings = embedding_model.encode(texts, show_progress_bar=False)\n",
        "            \n",
        "            # Create points\n",
        "            for j, (chunk, embedding) in enumerate(zip(batch_chunks, embeddings)):\n",
        "                point_id = str(uuid.uuid4())\n",
        "                points.append(\n",
        "                    PointStruct(\n",
        "                        id=point_id,\n",
        "                        vector=embedding.tolist(),\n",
        "                        payload={\n",
        "                            \"text\": chunk.page_content,\n",
        "                            \"metadata\": chunk.metadata,\n",
        "                            \"chunk_id\": chunk.metadata.get(\"chunk_id\", f\"{i+j}\"),\n",
        "                            \"source\": chunk.metadata.get(\"source_file\")\n",
        "                        }\n",
        "                    )\n",
        "                )\n",
        "        \n",
        "        # Upload all points to Qdrant\n",
        "        print(f\"\\nUploading {len(points)} points to local Qdrant...\")\n",
        "        qdrant_client.upsert(\n",
        "            collection_name=collection_name,\n",
        "            points=points\n",
        "        )\n",
        "        \n",
        "        print(f\"✓ Successfully uploaded {len(points)} chunks to local Qdrant!\")\n",
        "    \n",
        "    # Show final collection info\n",
        "    collection_info = qdrant_client.get_collection(collection_name=collection_name)\n",
        "    print(f\"\\nCollection info:\")\n",
        "    print(f\"  - Points count: {collection_info.points_count}\")\n",
        "    print(f\"  - Vector size: {collection_info.config.params.vectors.size}\")\n",
        "else:\n",
        "    print(\"⚠ No chunks to upload. Please load documents first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Retrieval, Prompt Enrichment, and Generation\n",
        "\n",
        "In this part, we move from preparing the knowledge base to actively using it in a RAG workflow. Everything runs locally!\n",
        "\n",
        "## 4.1: Implement Semantic Search\n",
        "\n",
        "With our knowledge base populated, let's build the **search function** that retrieves the most relevant document chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Semantic Search Works (Local)\n",
        "\n",
        "Here's what happens when you search the knowledge base:\n",
        "\n",
        "#### Step 1: Query Embedding\n",
        "- **Input**: Your question (text string)\n",
        "- **Process**: Local embedding model converts question → 384-dim vector\n",
        "- **Location**: Generated in RAM, not stored\n",
        "- **Time**: ~10-50ms\n",
        "\n",
        "#### Step 2: Vector Similarity Search\n",
        "- **Input**: Query vector (384 dimensions)\n",
        "- **Process**: Qdrant searches for most similar vectors using cosine similarity\n",
        "- **Algorithm**: Approximate Nearest Neighbor (ANN) search\n",
        "- **Location**: Searches in Qdrant (RAM or disk, depending on storage type)\n",
        "- **Time**: ~1-10ms per search (very fast!)\n",
        "\n",
        "#### Step 3: Result Retrieval\n",
        "- **Input**: Top-K similar vector IDs\n",
        "- **Process**: Qdrant retrieves document text and metadata\n",
        "- **Output**: List of documents with similarity scores\n",
        "- **Storage**: Retrieved from Qdrant payload (text + metadata stored with each vector)\n",
        "\n",
        "#### Data Flow:\n",
        "```\n",
        "Question → Embed (384-dim) → Search Qdrant → Retrieve Docs → Return Results\n",
        "```\n",
        "\n",
        "**Why It's Fast**:\n",
        "- Embedding: Single vector generation (~10-50ms)\n",
        "- Search: Optimized vector similarity search (~1-10ms)\n",
        "- No network calls: Everything is local\n",
        "- Total time: ~20-100ms per query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Query: What is SatcomLLM?\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Result 1 (Score: 0.5673) ---\n",
            "Section: Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning > References\n",
            "Text preview: Politi Marcello is a machine learning scientist and the leader of the SatComLLM project, funded by ESA under the ARTES program. Riccardo Corrente is a deep learning scientist at Pi School and played a key role in building both the large and small versions of SatComLLM as part of the Pi School team. ...\n",
            "\n",
            "--- Result 2 (Score: 0.2614) ---\n",
            "Section: Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples > References\n",
            "Text preview: * [1] [PERSON]. [PERSON], [PERSON], [PERSON], [PERSON], [PERSON], [PERSON], and [PERSON], \\\"Leo satellite networks assisted geo-distributed data processing,\\\" _IEEE Wireless Communications Letters_, 2024.\n",
            "* [2] [PERSON], [PERSON], [PERSON], [PERSON], [PERSON], [PERSON], [PERSON], and [PERSON], \\\"Gra...\n",
            "\n",
            "--- Result 3 (Score: 0.2393) ---\n",
            "Section: Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples > References\n",
            "Text preview: * [59] [PERSON]. [PERSON], [PERSON], [PERSON], [PERSON], [PERSON], [PERSON], and [PERSON], \\\"Hierarchical split federated learning: Convergence analysis and system optimization,\\\" _arXiv preprint arXiv:2412.07197_, 2024.\n",
            "* [60] [PERSON], [PERSON], [PERSON], [PERSON], and [PERSON], \\\"LCFed: An Effici...\n",
            "\n",
            "================================================================================\n",
            "✓ Local search function working correctly!\n"
          ]
        }
      ],
      "source": [
        "def search_knowledge_base(query, top_k=3):\n",
        "    \"\"\"\n",
        "    Search the knowledge base for relevant documents (fully local).\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        top_k: Number of top results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of relevant documents with scores\n",
        "    \"\"\"\n",
        "    # Embed the query (local, no API call!)\n",
        "    query_embedding = embedding_model.encode(query)\n",
        "    \n",
        "    # Search in local Qdrant\n",
        "    search_results = qdrant_client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=query_embedding.tolist(),\n",
        "        limit=top_k\n",
        "    )\n",
        "    \n",
        "    # Format results\n",
        "    results = []\n",
        "    for result in search_results.points:\n",
        "        results.append({\n",
        "            \"text\": result.payload[\"text\"],\n",
        "            \"metadata\": result.payload[\"metadata\"],\n",
        "            \"score\": result.score\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test the search function\n",
        "if chunks:\n",
        "    test_query = \"What is SatcomLLM?\"\n",
        "    print(f\"Test Query: {test_query}\\n\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    results = search_knowledge_base(test_query, top_k=3)\n",
        "    \n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"\\n--- Result {i} (Score: {result['score']:.4f}) ---\")\n",
        "        print(f\"Section: {result['metadata'].get('Header 1', '')} > {result['metadata'].get('Header 2', '')}\")\n",
        "        print(f\"Text preview: {result['text'][:300]}...\")\n",
        "        \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"✓ Local search function working correctly!\")\n",
        "else:\n",
        "    print(\"⚠ No chunks available for search. Please load documents first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Complete Local RAG Pipeline\n",
        "\n",
        "Now we bring everything together! This is the **main RAG function** that combines local retrieval with local generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete RAG Pipeline: Data Flow & Storage\n",
        "\n",
        "Here's the complete end-to-end process when you ask a question:\n",
        "\n",
        "#### Phase 1: Retrieval (Local)\n",
        "```\n",
        "User Question\n",
        "    ↓\n",
        "[Embedding Model] → 384-dim vector (generated in RAM, not stored)\n",
        "    ↓\n",
        "[Qdrant Search] → Find top-K similar vectors (searches local DB)\n",
        "    ↓\n",
        "[Retrieve Documents] → Get text + metadata from Qdrant payload\n",
        "    ↓\n",
        "Retrieved Context (in RAM)\n",
        "```\n",
        "\n",
        "**Storage Involved**:\n",
        "- Query embedding: Generated on-the-fly, not stored\n",
        "- Vector search: Searches Qdrant (RAM or `./qdrant_db/`)\n",
        "- Retrieved docs: Loaded into RAM temporarily\n",
        "\n",
        "#### Phase 2: Prompt Construction (In-Memory)\n",
        "```\n",
        "Retrieved Context + User Question\n",
        "    ↓\n",
        "[Format Prompt] → Create enriched prompt string\n",
        "    ↓\n",
        "Enriched Prompt (in RAM)\n",
        "```\n",
        "\n",
        "**Storage**: All in RAM, temporary\n",
        "\n",
        "#### Phase 3: Generation (Local LLM)\n",
        "```\n",
        "Enriched Prompt\n",
        "    ↓\n",
        "[Tokenization] → Convert text to token IDs (in RAM)\n",
        "    ↓\n",
        "[Local LLM Model] → Generate tokens (uses MPS/CPU/GPU memory)\n",
        "    ↓\n",
        "[Detokenization] → Convert tokens back to text\n",
        "    ↓\n",
        "Final Answer (returned to user)\n",
        "```\n",
        "\n",
        "**Storage Involved**:\n",
        "- Model weights: Loaded in RAM/GPU memory (from `~/.cache/huggingface/`)\n",
        "- Tokens: Generated in RAM\n",
        "- Final answer: Returned as string\n",
        "\n",
        "#### Complete Data Flow:\n",
        "```\n",
        "Question → Embed → Search Qdrant → Retrieve → Format Prompt → \n",
        "    Local LLM → Generate → Answer\n",
        "```\n",
        "\n",
        "**Total Storage Summary**:\n",
        "- **Models**: ~2.3GB in `~/.cache/huggingface/` (persistent)\n",
        "- **Chunks Cache**: ~1-5MB in `./chunks_cache/` (persistent)\n",
        "- **Vector DB**: Variable in RAM or `./qdrant_db/` (persistent if file-based)\n",
        "- **Runtime Memory**: ~2-4GB RAM for models + ~100-500MB for processing\n",
        "\n",
        "**Performance**:\n",
        "- Retrieval: ~20-100ms\n",
        "- Prompt formatting: ~1-5ms\n",
        "- LLM generation: ~1-5 seconds (depends on model size and max_tokens)\n",
        "- **Total**: ~1-6 seconds per query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Complete Local RAG Pipeline\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Question: What is SatcomLLM?\n",
            "================================================================================\n",
            "\n",
            "Searching local knowledge base...\n",
            "\n",
            "Retrieved 3 relevant documents:\n",
            "  [1] Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning > References (score: 0.567)\n",
            "  [2] Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples > References (score: 0.261)\n",
            "  [3] Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples > References (score: 0.239)\n",
            "\n",
            "Generating answer with local LLM...\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "ANSWER:\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "SatcomLLM is a machine learning project that uses deep learning techniques to analyze satellite imagery and develop models for satellite communications. The project aims to develop advanced models for satellite communications, combining domain-specific reasoning and question-answering capabilities. The team emphasizes collaboration, technical rigor, and innovation in SatCom AI solutions, supported by a cross-functional team of engineers, researchers, and domain experts. The project has been funded by ESA under the ARTES program and has led to the development of advanced models for satellite communications.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def ask_rag_question(question, top_k=3, max_new_tokens=256, temperature=0.7, show_sources=True):\n",
        "    \"\"\"\n",
        "    Complete local RAG pipeline: Retrieve relevant documents and generate answer using local LLM.\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        top_k: Number of documents to retrieve\n",
        "        max_new_tokens: Maximum tokens for generation\n",
        "        temperature: Sampling temperature\n",
        "        show_sources: Whether to display retrieved sources\n",
        "    \n",
        "    Returns:\n",
        "        Generated answer\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # Step 1: Retrieve relevant documents (local)\n",
        "    print(\"Searching local knowledge base...\")\n",
        "    retrieved_docs = search_knowledge_base(question, top_k=top_k)\n",
        "    \n",
        "    if not retrieved_docs:\n",
        "        return \"No relevant documents found in the knowledge base.\"\n",
        "    \n",
        "    if show_sources:\n",
        "        print(f\"\\nRetrieved {len(retrieved_docs)} relevant documents:\")\n",
        "        for i, doc in enumerate(retrieved_docs, 1):\n",
        "            headers = doc['metadata'].get('Header 1', '')\n",
        "            if doc['metadata'].get('Header 2'):\n",
        "                headers += f\" > {doc['metadata'].get('Header 2')}\"\n",
        "            print(f\"  [{i}] {headers} (score: {doc['score']:.3f})\")\n",
        "    \n",
        "    # Step 2: Format context from retrieved documents\n",
        "    context_parts = []\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        headers = []\n",
        "        if doc['metadata'].get('Header 1'):\n",
        "            headers.append(doc['metadata']['Header 1'])\n",
        "        if doc['metadata'].get('Header 2'):\n",
        "            headers.append(doc['metadata']['Header 2'])\n",
        "        \n",
        "        section_info = \" > \".join(headers) if headers else \"General\"\n",
        "        context_parts.append(f\"[Document {i}] {section_info}\\n{doc['text']}\\n\")\n",
        "    \n",
        "    context = \"\\n\".join(context_parts)\n",
        "    \n",
        "    # Step 3: Build enriched prompt\n",
        "    enriched_prompt = f\"\"\"You are a helpful assistant specializing in satellite communications.\n",
        "\n",
        "Use the following context from the documentation to answer the question accurately and concisely.\n",
        "If the answer cannot be found in the context, say so clearly.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "    \n",
        "    # Step 4: Generate answer using local LLM\n",
        "    print(\"\\nGenerating answer with local LLM...\")\n",
        "    answer = chat_with_local_llm(\n",
        "        enriched_prompt, \n",
        "        max_new_tokens=max_new_tokens, \n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    # Display answer\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(f\"ANSWER:\")\n",
        "    print(f\"{'─'*80}\")\n",
        "    print(answer)\n",
        "    print(f\"\\n{'='*80}\\n\")\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# Test the complete local RAG pipeline\n",
        "if chunks:\n",
        "    print(\"Testing Complete Local RAG Pipeline\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Test question\n",
        "    ask_rag_question(\n",
        "        \"What is SatcomLLM?\",\n",
        "        top_k=3,\n",
        "        temperature=0.7,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "else:\n",
        "    print(\"⚠ No chunks available. Please load documents first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Testing with Custom Questions\n",
        "\n",
        "Let's test our local RAG system with several questions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Question: What is a LEO satellite?\n",
            "================================================================================\n",
            "\n",
            "Searching local knowledge base...\n",
            "\n",
            "Retrieved 3 relevant documents:\n",
            "  [1] Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples > References (score: 0.509)\n",
            "  [2] Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples (score: 0.503)\n",
            "  [3] Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples > I Introduction (score: 0.496)\n",
            "\n",
            "Generating answer with local LLM...\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "ANSWER:\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "A LEO satellite is a type of non-terrestrial network (NTN) that orbits at an altitude of less than 3500 km above Earth's surface.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Question: What is deep learning?\n",
            "================================================================================\n",
            "\n",
            "Searching local knowledge base...\n",
            "\n",
            "Retrieved 3 relevant documents:\n",
            "  [1] Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning > III Methods (score: 0.447)\n",
            "  [2] Constructing 4D Radio Map in LEO Satellite Networks with Limited Samples > VII Conclusion (score: 0.436)\n",
            "  [3] Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning > I Introduction (score: 0.412)\n",
            "\n",
            "Generating answer with local LLM...\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "ANSWER:\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Deep learning is a type of artificial neural network (ANN) that consists of multiple layers of neurons that can learn complex patterns and functions from large amounts of data. In this context, deep learning refers to the use of these networks in the analysis of satellite imagery for wildfire detection.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Question: What is Internet of Things?\n",
            "================================================================================\n",
            "\n",
            "Searching local knowledge base...\n",
            "\n",
            "Retrieved 3 relevant documents:\n",
            "  [1] A Survey of Energy Efficient Schemes in Ad-hoc Networks > References (score: 0.404)\n",
            "  [2] A Survey of Energy Efficient Schemes in Ad-hoc Networks > References (score: 0.397)\n",
            "  [3] A Survey of Energy Efficient Schemes in Ad-hoc Networks > References (score: 0.370)\n",
            "\n",
            "Generating answer with local LLM...\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "ANSWER:\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Internet of Things (IoT) refers to the network of interconnected devices, appliances, and sensors that enable the physical world to be monitored and controlled remotely through the internet. IoT devices can be used in various industries, such as healthcare, transportation, and manufacturing, to improve efficiency, reduce costs, and improve safety.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with more questions\n",
        "if chunks:\n",
        "    test_questions = [\n",
        "        \"What is a LEO satellite?\",\n",
        "        \"What is deep learning?\",\n",
        "        \"What is Internet of Things?\"\n",
        "    ]\n",
        "    \n",
        "    for question in test_questions:\n",
        "        ask_rag_question(question, top_k=3, temperature=0.7, max_new_tokens=256)\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "else:\n",
        "    print(\"⚠ No chunks available. Please load documents first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Storage Management & Cleanup\n",
        "\n",
        "#### Checking Your Storage Usage\n",
        "\n",
        "You can check where models are stored and how much space they use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace cache: /Users/riccardocorrente/.cache/huggingface/hub\n",
            "Cache exists: True\n",
            "Total cache size: 4.27 GB\n",
            "\n",
            "Chunks cache: chunks_cache\n",
            "Chunks cache size: 0.18 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Get HuggingFace cache location (compatible with all versions)\n",
        "try:\n",
        "    from huggingface_hub.constants import HF_HUB_CACHE\n",
        "    cache_path = Path(HF_HUB_CACHE)\n",
        "except ImportError:\n",
        "    # Fallback for different API versions\n",
        "    cache_path = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
        "\n",
        "print(f\"HuggingFace cache: {cache_path}\")\n",
        "print(f\"Cache exists: {cache_path.exists()}\")\n",
        "\n",
        "if cache_path.exists():\n",
        "    # Calculate total size\n",
        "    total_size = sum(f.stat().st_size for f in cache_path.rglob('*') if f.is_file())\n",
        "    print(f\"Total cache size: {total_size / (1024**3):.2f} GB\")\n",
        "\n",
        "# Check chunks cache\n",
        "chunks_cache_size = sum(f.stat().st_size for f in CHUNKS_CACHE_DIR.rglob('*') if f.is_file())\n",
        "print(f\"\\nChunks cache: {CHUNKS_CACHE_DIR}\")\n",
        "print(f\"Chunks cache size: {chunks_cache_size / (1024**2):.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS available: True\n",
            "MPS built: True\n",
            "\n",
            "Total System RAM (shared with GPU): 18.0 GB\n",
            "MPS memory currently allocated: 2.14 GB\n",
            "MPS driver allocated memory: 6.07 GB\n",
            "\n",
            "--- System Memory Details ---\n",
            "Mach Virtual Memory Statistics: (page size of 16384 bytes)\n",
            "Pages free:                                5232.\n",
            "Pages active:                            187914.\n",
            "Pages inactive:                          186178.\n",
            "Pages speculative:                          693.\n",
            "Pages throttled:                              0.\n",
            "Pages wired down:                        200909.\n",
            "Pages purgeable:                            696.\n",
            "\"Translation faults\":                 658593749.\n",
            "Pages copy-on-write:                   11289929.\n",
            "Pages zero filled:                    325555858.\n",
            "Pages reactivated:                    138520233.\n",
            "Pages purged:                          29368298.\n",
            "File-backed pages:                       100936.\n",
            "Anonymous pages:                         273849.\n",
            "Pages stored in compressor:             1531014.\n",
            "Pages occupied by compressor:            558001.\n",
            "Decompressions:                       179023232.\n",
            "Compressions:                         203441475.\n",
            "Pageins:                                9285815.\n",
            "Pageouts:                                328140.\n",
            "Swapins:                                2932799.\n",
            "Swapouts:                               4740904.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import subprocess\n",
        "\n",
        "# Check if MPS is available\n",
        "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
        "\n",
        "# Get total system memory (Apple Silicon uses unified memory - shared between CPU and GPU)\n",
        "import os\n",
        "total_ram = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024**3)\n",
        "print(f\"\\nTotal System RAM (shared with GPU): {total_ram:.1f} GB\")\n",
        "\n",
        "# Check current memory allocation on MPS\n",
        "if torch.backends.mps.is_available():\n",
        "    # Current MPS memory allocated by PyTorch\n",
        "    allocated = torch.mps.current_allocated_memory() / (1024**3)\n",
        "    print(f\"MPS memory currently allocated: {allocated:.2f} GB\")\n",
        "    \n",
        "    # Driver-level memory allocated\n",
        "    driver_allocated = torch.mps.driver_allocated_memory() / (1024**3)\n",
        "    print(f\"MPS driver allocated memory: {driver_allocated:.2f} GB\")\n",
        "\n",
        "# Get detailed system memory info using macOS command\n",
        "print(\"\\n--- System Memory Details ---\")\n",
        "result = subprocess.run(['vm_stat'], capture_output=True, text=True)\n",
        "print(result.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Managing Storage\n",
        "\n",
        "**To Free Up Space**:\n",
        "1. **Delete specific models**: Remove folders from `~/.cache/huggingface/models--*/`\n",
        "2. **Clear entire cache**: Delete `~/.cache/huggingface/` (models will re-download when needed)\n",
        "3. **Clear chunks cache**: Delete `./chunks_cache/` folder (chunks will be re-processed)\n",
        "4. **Clear Qdrant DB**: Delete `./qdrant_db/` folder (if using persistent storage)\n",
        "\n",
        "**To Persist Data Between Sessions**:\n",
        "1. Use persistent Qdrant storage: `QdrantClient(path=\"./qdrant_db\")`\n",
        "2. Models are automatically cached by HuggingFace\n",
        "3. Chunks are automatically cached in `./chunks_cache/`\n",
        "4. Documents in `data/` folder are your source files\n",
        "\n",
        "#### Recommended Setup for Production\n",
        "\n",
        "```python\n",
        "# Persistent Qdrant storage\n",
        "qdrant_client = QdrantClient(path=\"./qdrant_db\")\n",
        "\n",
        "# Chunks automatically cached in ./chunks_cache/\n",
        "# Models automatically cached by HuggingFace\n",
        "# Documents in data/ folder\n",
        "```\n",
        "\n",
        "This way:\n",
        "- Models persist (HuggingFace cache)\n",
        "- Chunks persist (`./chunks_cache/`)\n",
        "- Vector database persists (`./qdrant_db/`)\n",
        "- Documents persist (`data/` folder)\n",
        "- Everything survives notebook restarts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Complete Local RAG Pipeline\n",
        "\n",
        "Congratulations! You've built a complete **fully local** RAG system with:\n",
        "\n",
        "### Components:\n",
        "1. **Document Processing**: Markdown chunking with header-based splitting\n",
        "2. **Chunk Caching**: Local JSON files for fast re-loading (NEW!)\n",
        "3. **Embeddings**: Local sentence-transformers (all-MiniLM-L6-v2)\n",
        "4. **Vector Database**: Local Qdrant (in-memory or file-based)\n",
        "5. **LLM Generation**: Local TinyLlama model\n",
        "6. **RAG Orchestration**: Custom pipeline combining retrieval and generation\n",
        "\n",
        "### Key Advantages of Local Setup:\n",
        "- **No API costs** - Everything runs on your machine\n",
        "- **Privacy** - Your data never leaves your computer\n",
        "- **Offline capable** - Works without internet\n",
        "- **Fast** - No network latency\n",
        "- **Customizable** - Full control over models and parameters\n",
        "- **Persistent chunks** - Skip re-processing with local cache (NEW!)\n",
        "\n",
        "### Storage Locations Summary:\n",
        "\n",
        "| Component | Location | Size | Persistent? |\n",
        "|-----------|----------|------|-------------|\n",
        "| **LLM Model** | `~/.cache/huggingface/models--TinyLlama--.../` | ~2.2GB | Yes |\n",
        "| **Embedding Model** | `~/.cache/huggingface/sentence-transformers/` | ~80MB | Yes |\n",
        "| **Chunks Cache** | `./chunks_cache/*.json` | ~1-5MB | Yes |\n",
        "| **Vector DB** | RAM or `./qdrant_db/` | ~10-200MB | If file-based |\n",
        "| **Source Docs** | `./data/*.md` | Variable | Yes |\n",
        "\n",
        "### Next Steps:\n",
        "- Try different questions\n",
        "- Adjust top_k for more/less context\n",
        "- Experiment with temperature settings\n",
        "- Add more documents to the knowledge base\n",
        "- Switch to larger models if you have more RAM\n",
        "- Use persistent Qdrant storage for larger datasets\n",
        "- Share your chunks cache between projects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
